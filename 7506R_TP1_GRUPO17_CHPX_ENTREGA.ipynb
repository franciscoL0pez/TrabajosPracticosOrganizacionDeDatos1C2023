{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/franlopez1234/7506R-1C2023-GRUPO17/blob/main/7506R_TP1_GRUPO17_CHPX_ENTREGA_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjquUrdmJJIg"
   },
   "source": [
    "##Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "j06w1QqlJLiY"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn import tree\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score, precision_score,\n",
    "                             recall_score, make_scorer)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold, KFold, RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3OOn8nICCW6r"
   },
   "source": [
    "##Realizamos el tratamiento de las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BQGfbgFiz7ry"
   },
   "outputs": [],
   "source": [
    "df_hotels_train = pd.read_csv('hotels_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "id": "D3-rrHE7Cu7O",
    "outputId": "97309757-5526-4970-9992-7acac05fbe76"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hotel</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>arrival_date_year</th>\n",
       "      <th>arrival_date_month</th>\n",
       "      <th>arrival_date_week_number</th>\n",
       "      <th>arrival_date_day_of_month</th>\n",
       "      <th>stays_in_weekend_nights</th>\n",
       "      <th>stays_in_week_nights</th>\n",
       "      <th>adults</th>\n",
       "      <th>children</th>\n",
       "      <th>...</th>\n",
       "      <th>company</th>\n",
       "      <th>days_in_waiting_list</th>\n",
       "      <th>customer_type</th>\n",
       "      <th>adr</th>\n",
       "      <th>required_car_parking_spaces</th>\n",
       "      <th>total_of_special_requests</th>\n",
       "      <th>reservation_status</th>\n",
       "      <th>reservation_status_date</th>\n",
       "      <th>id</th>\n",
       "      <th>is_canceled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>City Hotel</td>\n",
       "      <td>49</td>\n",
       "      <td>2016</td>\n",
       "      <td>September</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Transient</td>\n",
       "      <td>115.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Canceled</td>\n",
       "      <td>2016-08-25</td>\n",
       "      <td>7aa4cc6b-b92c-4061-b21d-4f9e0cac4689</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Resort Hotel</td>\n",
       "      <td>4</td>\n",
       "      <td>2015</td>\n",
       "      <td>October</td>\n",
       "      <td>44</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Transient</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Check-Out</td>\n",
       "      <td>2015-11-01</td>\n",
       "      <td>b1428f80-c56c-4ae4-91a9-6962edae08b1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          hotel  lead_time  arrival_date_year arrival_date_month  \\\n",
       "0    City Hotel         49               2016          September   \n",
       "1  Resort Hotel          4               2015            October   \n",
       "\n",
       "   arrival_date_week_number  arrival_date_day_of_month  \\\n",
       "0                        37                          5   \n",
       "1                        44                         31   \n",
       "\n",
       "   stays_in_weekend_nights  stays_in_week_nights  adults  children  ...  \\\n",
       "0                        1                     2       1       0.0  ...   \n",
       "1                        0                     1       2       0.0  ...   \n",
       "\n",
       "   company days_in_waiting_list customer_type    adr  \\\n",
       "0      NaN                    0     Transient  115.5   \n",
       "1      NaN                    0     Transient   42.0   \n",
       "\n",
       "  required_car_parking_spaces  total_of_special_requests  reservation_status  \\\n",
       "0                           0                          1            Canceled   \n",
       "1                           1                          0           Check-Out   \n",
       "\n",
       "   reservation_status_date                                    id is_canceled  \n",
       "0               2016-08-25  7aa4cc6b-b92c-4061-b21d-4f9e0cac4689           1  \n",
       "1               2015-11-01  b1428f80-c56c-4ae4-91a9-6962edae08b1           0  \n",
       "\n",
       "[2 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hotels_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bwpp1z_hCvOh",
    "outputId": "c5f3985e-1210-48ca-9d38-8142417c4fe2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hotel                              object\n",
       "lead_time                           int64\n",
       "arrival_date_year                   int64\n",
       "arrival_date_month                 object\n",
       "arrival_date_week_number            int64\n",
       "arrival_date_day_of_month           int64\n",
       "stays_in_weekend_nights             int64\n",
       "stays_in_week_nights                int64\n",
       "adults                              int64\n",
       "children                          float64\n",
       "babies                              int64\n",
       "meal                               object\n",
       "country                            object\n",
       "market_segment                     object\n",
       "distribution_channel               object\n",
       "is_repeated_guest                   int64\n",
       "previous_cancellations              int64\n",
       "previous_bookings_not_canceled      int64\n",
       "reserved_room_type                 object\n",
       "assigned_room_type                 object\n",
       "booking_changes                     int64\n",
       "deposit_type                       object\n",
       "agent                             float64\n",
       "company                           float64\n",
       "days_in_waiting_list                int64\n",
       "customer_type                      object\n",
       "adr                               float64\n",
       "required_car_parking_spaces         int64\n",
       "total_of_special_requests           int64\n",
       "id                                 object\n",
       "is_canceled                         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Eliminamos las columnas de reservation_status y reservation_status_date para evitar confusiones\n",
    "df_hotels_train = df_hotels_train.drop(['reservation_status','reservation_status_date'], axis =1) #->Desmarcar al abrir el archivo\n",
    "df_hotels_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MnkGgXHxCvsA"
   },
   "outputs": [],
   "source": [
    "df_hotels_train['hotel'] = df_hotels_train['hotel'].astype('string')\n",
    "df_hotels_train['arrival_date_month'] = df_hotels_train['arrival_date_month'].astype('string')\n",
    "df_hotels_train['meal'] = df_hotels_train['meal'].astype('string')\n",
    "df_hotels_train['country'] = df_hotels_train['country'].astype('string')\n",
    "df_hotels_train['id'] = df_hotels_train['id'].astype('string')\n",
    "df_hotels_train['customer_type'] = df_hotels_train['customer_type'].astype('string')\n",
    "df_hotels_train['market_segment'] = df_hotels_train['market_segment'].astype('string')\n",
    "df_hotels_train['distribution_channel'] = df_hotels_train['distribution_channel'].astype('string')\n",
    "df_hotels_train['reserved_room_type'] = df_hotels_train['reserved_room_type'].astype('string')\n",
    "df_hotels_train['assigned_room_type'] = df_hotels_train['assigned_room_type'].astype('string')\n",
    "df_hotels_train['deposit_type'] = df_hotels_train['deposit_type'].astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W44Yo-SfDpJ3"
   },
   "source": [
    "Retiramos los nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "J3nelv_nCwJe"
   },
   "outputs": [],
   "source": [
    "#Company\n",
    "df_hotels_train['company'].fillna(-9999, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Lnm3sVFgDtAu"
   },
   "outputs": [],
   "source": [
    "#Agent\n",
    "df_hotels_train['agent'].fillna(-9999, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zvjsw1dBDu0u"
   },
   "outputs": [],
   "source": [
    "#Country\n",
    "df_hotels_train['country'].fillna('PRT', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Q6bMvYh8Dwgx"
   },
   "outputs": [],
   "source": [
    "#Children\n",
    "df_hotels_train['children'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vO6eiYGXEAxV"
   },
   "source": [
    "Analizamos los outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "Z9ev6iV_Ka22",
    "outputId": "18ed9c33-c5ad-4a12-e9b7-99007c22bf7b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>z_lead_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.186066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>3.402834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>3.086713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>3.691859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>4.640223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61579</th>\n",
       "      <td>3.655731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61695</th>\n",
       "      <td>3.213162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61761</th>\n",
       "      <td>3.466059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61783</th>\n",
       "      <td>3.086713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61790</th>\n",
       "      <td>3.845404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>732 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       z_lead_time\n",
       "15        3.186066\n",
       "51        3.402834\n",
       "95        3.086713\n",
       "426       3.691859\n",
       "576       4.640223\n",
       "...            ...\n",
       "61579     3.655731\n",
       "61695     3.213162\n",
       "61761     3.466059\n",
       "61783     3.086713\n",
       "61790     3.845404\n",
       "\n",
       "[732 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#columnas:(lead_time, stays_in_weekend_nights, stays_in_week_nights, adults, children, babies, previous_cancellations, previous_bookings_not_canceled, booking_changes, days_in_waiting_list, required_car_parking_spaces)\n",
    "media_lead_time=np.mean(df_hotels_train.lead_time)\n",
    "std_lead_time=np.std(df_hotels_train.lead_time)\n",
    "df_zscore = pd.DataFrame()\n",
    "df_zscore['z_lead_time']=(df_hotels_train.lead_time - media_lead_time)/std_lead_time\n",
    "df_zscore[df_zscore['z_lead_time']>3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cej_pCnpECHS",
    "outputId": "c90cf065-3498-42df-b30a-4a25881bf484"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2g/pz89x6gd4_z44j2mr8rthvmh0000gn/T/ipykernel_2604/3445656125.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_hotels_train.lead_time[i] = round(media_lead_time)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df_zscore['z_lead_time'])):\n",
    "  if df_zscore.z_lead_time[i] > 3:\n",
    "    df_hotels_train.lead_time[i] = round(media_lead_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DOGcrc26EMFE",
    "outputId": "1fa903a5-e206-41ec-f316-1384f5993be1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2g/pz89x6gd4_z44j2mr8rthvmh0000gn/T/ipykernel_2604/1879383880.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_hotels_train.previous_cancellations[i] = 0 #Redondeamos a 0 ya que no tiene sentido normilazarlos sacando una media\n"
     ]
    }
   ],
   "source": [
    "#Normalizamos previus cancellation\n",
    "for i in range(len(df_hotels_train.previous_cancellations)):\n",
    "  if df_hotels_train.previous_cancellations[i] > 10:\n",
    "    df_hotels_train.previous_cancellations[i] = 0 #Redondeamos a 0 ya que no tiene sentido normilazarlos sacando una media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0E0lRltOEScJ",
    "outputId": "cbac7e97-6bc6-48b7-dee9-028aedee9983"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2g/pz89x6gd4_z44j2mr8rthvmh0000gn/T/ipykernel_2604/3966743444.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_hotels_train.previous_bookings_not_canceled[i] = round(media_previous_bookings_not_canceled)\n"
     ]
    }
   ],
   "source": [
    "media_previous_bookings_not_canceled=np.mean(df_hotels_train.previous_bookings_not_canceled)\n",
    "for i in range(len(df_hotels_train.previous_bookings_not_canceled)):\n",
    "  if df_hotels_train.previous_bookings_not_canceled[i] > 30:\n",
    "    df_hotels_train.previous_bookings_not_canceled[i] = round(media_previous_bookings_not_canceled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Srd6c_rEETU8",
    "outputId": "bd59f0a7-3266-4bd8-bdde-03d3ad2d8495"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2g/pz89x6gd4_z44j2mr8rthvmh0000gn/T/ipykernel_2604/3893981792.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_hotels_train.booking_changes[i] = 0 #Redondeamos a 0 ya que no tiene sentido normilazarlos sacando una media\n"
     ]
    }
   ],
   "source": [
    "#Normalizamos los valores atipicos de booking change\n",
    "for i in range(len(df_hotels_train.booking_changes)):\n",
    "  if df_hotels_train.booking_changes[i] > 10:\n",
    "    df_hotels_train.booking_changes[i] = 0 #Redondeamos a 0 ya que no tiene sentido normilazarlos sacando una media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QdrLoqUQEXJ9",
    "outputId": "516f1f35-ba00-458c-eb5a-2c68885edc0f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2g/pz89x6gd4_z44j2mr8rthvmh0000gn/T/ipykernel_2604/3411085939.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_hotels_train.days_in_waiting_list[i] = round(media_days_in_waiting_list)\n"
     ]
    }
   ],
   "source": [
    "media_days_in_waiting_list = np.mean(df_hotels_train.days_in_waiting_list)\n",
    "\n",
    "for i in range(len(df_hotels_train.days_in_waiting_list)):\n",
    "  if df_hotels_train.days_in_waiting_list[i] > 100:\n",
    "    df_hotels_train.days_in_waiting_list[i] = round(media_days_in_waiting_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M46EN1VPEZ0n",
    "outputId": "59a3adbd-b07a-4acb-cc0d-010ee9f9e2db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2g/pz89x6gd4_z44j2mr8rthvmh0000gn/T/ipykernel_2604/2146687091.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_hotels_train.required_car_parking_spaces[i] = round(media_required_car_parking_spaces)\n"
     ]
    }
   ],
   "source": [
    "media_required_car_parking_spaces = np.mean(df_hotels_train.required_car_parking_spaces)\n",
    "\n",
    "for i in range(len(df_hotels_train.required_car_parking_spaces)):\n",
    "  if df_hotels_train.required_car_parking_spaces[i] > 7:\n",
    "    df_hotels_train.required_car_parking_spaces[i] = round(media_required_car_parking_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "b81_ya2DEmz7"
   },
   "outputs": [],
   "source": [
    "\n",
    "df_hotels_train.reset_index(drop=True, inplace=True)\n",
    "for i in range(len(df_hotels_train)):\n",
    "  if df_hotels_train.adults[i] == 0:\n",
    "    df_hotels_train = df_hotels_train.drop(index = [i])\n",
    "\n",
    "df_hotels_train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "mWK1RzFJEoI9"
   },
   "outputs": [],
   "source": [
    "df_hotels_train.reset_index(drop=True, inplace=True)\n",
    "for i in range(len(df_hotels_train)):\n",
    "  if df_hotels_train.adults[i] <= 2 and df_hotels_train.babies[i] >= 8:\n",
    "    df_hotels_train = df_hotels_train.drop(index = [i])\n",
    "    \n",
    "\n",
    "df_hotels_train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UZjV3q36EolQ",
    "outputId": "da34d08e-64fa-44e6-809d-c7892b614fd1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2g/pz89x6gd4_z44j2mr8rthvmh0000gn/T/ipykernel_2604/842624944.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_hotels_train.adults[i] = round(media_adults)\n"
     ]
    }
   ],
   "source": [
    "media_adults=np.mean(df_hotels_train.adults)\n",
    "\n",
    "for i in range(len(df_hotels_train)):\n",
    "  if df_hotels_train.adults[i] >= 25:\n",
    "    df_hotels_train.adults[i] = round(media_adults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "IuPJ_YGBHlpx"
   },
   "outputs": [],
   "source": [
    "df_hotels_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "for i in range(len(df_hotels_train)):\n",
    "  if df_hotels_train.stays_in_week_nights[i] == 0 and df_hotels_train.stays_in_weekend_nights[i] == 0:\n",
    "    df_hotels_train = df_hotels_train.drop(index = [i])\n",
    "\n",
    "df_hotels_train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FDsjQFAEyb2"
   },
   "source": [
    "Eliminamos id (recordar que la vamos a necesitar mas tarde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Uy1F1_oHE1yh"
   },
   "outputs": [],
   "source": [
    "eliminar_columnas = ['id'] # -> Recordar que el id lo vamos a necesitar para machear en kaggle\n",
    "                                                                      \n",
    "df_hotels_train.drop(eliminar_columnas, axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPl9Pd2oFGFz"
   },
   "source": [
    "## Realizamos un encode de las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "L2xs9NYdFI_w"
   },
   "outputs": [],
   "source": [
    "# Realizamos el one hot encoding, con scikit learn.\n",
    "# Por cada columna a encodear, hacer un fit transform. Dropear la columna, y agregar las columnas nuevas con el nombre adecuado\n",
    "df_hotels_train_to_encode = df_hotels_train.copy()\n",
    "cols_a_encodear = ['hotel','arrival_date_month','customer_type','meal', 'distribution_channel','market_segment', 'reserved_room_type', 'assigned_room_type', 'deposit_type', 'country']\n",
    "ohe = OneHotEncoder(drop='first', handle_unknown='infrequent_if_exist') # Siempre se elimina la primera columna, o el feature entero si solo tiene un valor (no aporta informacion).\n",
    "transformer = make_column_transformer(\n",
    "    (ohe, cols_a_encodear),\n",
    "    remainder='passthrough',\n",
    "    verbose_feature_names_out=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tr.joblib']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(transformer, \"tr.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxXxgeudyX0d"
   },
   "source": [
    "Dropeamos \"is_canceled\" ya que no es necesario encodearla y la guardamos en un dataset para utilizarla mas adelante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "3jmf4GM3FN2e"
   },
   "outputs": [],
   "source": [
    "df_target = df_hotels_train['is_canceled']\n",
    "hotels_train_encoded = transformer.fit_transform(df_hotels_train_to_encode.drop('is_canceled', axis=1)) # Se dropea is_canceled, porque no hace falta encodearlo, y ademas la instancia de test no va a tener esa columna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yw57Q-zu7Uzc"
   },
   "source": [
    "Encodeamos nuestras variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "BLljnI_iFPiv"
   },
   "outputs": [],
   "source": [
    "df_hotels_train_encoded = pd.DataFrame(\n",
    "    hotels_train_encoded.toarray(), \n",
    "    columns=transformer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "#Mostramos las primera 10 columnas de country, ya que el resto tiene una cantidad bajisima de datos y no aportan demasiado\n",
    "country_cols = df_hotels_train_encoded.columns[df_hotels_train_encoded.columns.str.startswith('country_')]\n",
    "selected_columns = (df_hotels_train_encoded[country_cols] != 0).sum()\n",
    "#top_cols = selected_columns.nlargest(10).index\n",
    "top_cols = ['country_PRT', 'country_GBR', 'country_FRA', 'country_ESP',\n",
    "       'country_DEU', 'country_ITA', 'country_IRL', 'country_BRA',\n",
    "       'country_BEL', 'country_USA']\n",
    "other_countries = [col for col in country_cols if col not in top_cols]\n",
    "df_hotels_train_encoded['country_other'] = df_hotels_train_encoded[other_countries].sum(axis=1)\n",
    "df_hotels_train_encoded.drop(other_countries, axis=1, inplace=True)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zn76efLw3Lm9"
   },
   "source": [
    "Dividimos el data set y copiamos 'is_canceled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "YvavAihVLadM"
   },
   "outputs": [],
   "source": [
    "#Creo un dataset con los features que voy a usar para clasificar\n",
    "df_hotels_train_x = df_hotels_train_encoded\n",
    "\n",
    "#Creo un dataset con la variable target\n",
    "df_hotels_train_y = df_hotels_train['is_canceled'].copy()\n",
    "\n",
    "#Genero los conjuntos de train y test]\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_hotels_train_x, \n",
    "                                                    df_hotels_train_y, \n",
    "                                                    test_size=0.3,  #proporcion 70/30\n",
    "                                                    random_state=2) #semilla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lok1LbNv3127"
   },
   "source": [
    "Creamos una funcion para mostrar nuestras metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Yxm5zPZrwiSJ"
   },
   "outputs": [],
   "source": [
    "def metricas(y_pred,y_test):\n",
    "\n",
    "  print(classification_report(y_test,y_pred))\n",
    "  \n",
    "  cm = confusion_matrix(y_test,y_pred)\n",
    "  sns.heatmap(cm, cmap='Blues',annot=True,fmt='g')\n",
    "  plt.xlabel('Predicted')\n",
    "  plt.ylabel('True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJNDhDoJPamf"
   },
   "source": [
    "##Redes Neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYhfzgDuZvCn"
   },
   "source": [
    "En esta etapa buscaremos crear una red neuronal para predecir nuestras cancelaciones de reserva.\n",
    "Luego, optimizaremos los hiper parametros para tratar de mejorar su performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2vfyRc4boDG"
   },
   "source": [
    "Importamos las librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "HK-LcTrzbjz3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-21 15:30:45.386509: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import f1_score,  recall_score, precision_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils.vis_utils import plot_model\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1) \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XA-fFKlAbr6f"
   },
   "source": [
    "Escalamos nuestros datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "beHsT0C-QB5u"
   },
   "outputs": [],
   "source": [
    "#Escalamos los datos de train de nuestro data set\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrRZysX2bv4l"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tw__fvUDaDoF",
    "outputId": "2139cbcd-1d94-4c42-a375-ea87b2a06530"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1)                 81        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 83\n",
      "Trainable params: 83\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-21 15:31:13.537672: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Calculamos la cantidad de clases\n",
    "cant_clases = len(np.unique(y_train)) #Nuestro target\n",
    "d_in = len(x_train.columns) #Parametros de entrenamiento\n",
    "\n",
    "model_reservas = keras.Sequential([\n",
    "   keras.layers.Dense(1,input_shape=(d_in,), activation='relu',kernel_initializer='uniform'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "\n",
    "#Mostramos los resultados\n",
    "model_reservas.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJNd9tXqhP4h"
   },
   "source": [
    "Compilamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SA_AX9V-vGtz",
    "outputId": "c9590d01-e61b-49c8-c070-912d4d8a2524"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "  \n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / (c2 + K.epsilon())\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / (c3 + K.epsilon())\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "uQSebtzihRSB"
   },
   "outputs": [],
   "source": [
    "model_reservas.compile(\n",
    "    optimizer = keras.optimizers.SGD(learning_rate = 0.001), \n",
    "    loss = 'binary_crossentropy', #Como mi salida es binaria uso binary_crossentropy\n",
    "\n",
    "    #Metrica que calcula en cada it \n",
    "\n",
    "    metrics = [f1_score]\n",
    ")\n",
    "\n",
    "cant_epochs = 100\n",
    "h_model_reservas = model_reservas.fit(x_train_scaled, y_train, epochs = cant_epochs, batch_size = 16, verbose = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "umNOpUlNgyYk",
    "outputId": "a25b94df-e6cc-430d-a5f7-ce35fd27b7d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "577/577 [==============================] - 0s 480us/step\n"
     ]
    }
   ],
   "source": [
    "y_predic_res = model_reservas.predict(x_test_scaled) #Realizamos el predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4LaOUS0Vg7oO",
    "outputId": "81b93716-487b-4ac3-c926-ee956994140c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18435, 1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predic_res.shape #Vemos el predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHyX0AV7xZcb"
   },
   "source": [
    "Graficamos la matriz de confusion y mostramos nuestras metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 636
    },
    "id": "RS-PNkarxYwM",
    "outputId": "12ff1bae-4656-438b-9446-b2ab9af1107d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "577/577 [==============================] - 0s 484us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.85      0.79      7848\n",
      "           1       0.88      0.76      0.82     10587\n",
      "\n",
      "    accuracy                           0.80     18435\n",
      "   macro avg       0.80      0.81      0.80     18435\n",
      "weighted avg       0.81      0.80      0.80     18435\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_predic_ = model_reservas.predict(x_test_scaled)\n",
    "y_predic_cat = np.where(y_predic_>0.4,1,0) #Seteamos un corte en los datos para determinar si cancelo o no \n",
    "\n",
    "ds_validacion=pd.DataFrame(y_predic_cat,y_test).reset_index() #Armamos un dataset\n",
    "ds_validacion.columns=['y_pred','y_real'] #Nombramos las columnas\n",
    "\n",
    "metricas(ds_validacion.y_pred,ds_validacion.y_real) #Dividimos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-21 18:31:10.950253: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-21 18:31:10.958069: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-21 18:31:10.965697: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-21 18:31:10.991276: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-21 18:31:11.015255: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-21 18:31:11.039846: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-21 18:31:14.373479: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-21 18:31:14.394561: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-21 18:31:14.394561: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-21 18:31:14.394889: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-21 18:31:14.395018: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-21 18:31:14.397373: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.4189 - accuracy: 0.7918\n",
      "Epoch 2/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.4289 - accuracy: 0.7843\n",
      "Epoch 2/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.4158 - accuracy: 0.7920\n",
      "Epoch 2/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.4091 - accuracy: 0.7996\n",
      "Epoch 2/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.4212 - accuracy: 0.7897\n",
      "  30/1076 [..............................] - ETA: 1s - loss: 0.3486 - accuracy: 0.8333Epoch 2/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.4133 - accuracy: 0.7946\n",
      "Epoch 2/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3649 - accuracy: 0.8202\n",
      "Epoch 3/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3597 - accuracy: 0.8263\n",
      "Epoch 3/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3600 - accuracy: 0.8227\n",
      "Epoch 3/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3580 - accuracy: 0.8250\n",
      "  30/1076 [..............................] - ETA: 1s - loss: 0.3449 - accuracy: 0.8427Epoch 3/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3579 - accuracy: 0.8244\n",
      "Epoch 3/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3561 - accuracy: 0.8247\n",
      "Epoch 3/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3488 - accuracy: 0.8287\n",
      "Epoch 4/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3419 - accuracy: 0.8308\n",
      "Epoch 4/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3442 - accuracy: 0.8317\n",
      "Epoch 4/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3438 - accuracy: 0.8328\n",
      "Epoch 4/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3426 - accuracy: 0.8313\n",
      "Epoch 4/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3400 - accuracy: 0.8326\n",
      "Epoch 4/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3412 - accuracy: 0.8337\n",
      "1070/1076 [============================>.] - ETA: 0s - loss: 0.3337 - accuracy: 0.8363Epoch 5/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3339 - accuracy: 0.8362\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3325 - accuracy: 0.8376\n",
      "Epoch 5/100\n",
      "Epoch 5/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3335 - accuracy: 0.8378\n",
      " 990/1076 [==========================>...] - ETA: 0s - loss: 0.3318 - accuracy: 0.8396Epoch 5/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3336 - accuracy: 0.8368\n",
      "Epoch 5/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3314 - accuracy: 0.8396\n",
      "Epoch 5/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3255 - accuracy: 0.8430\n",
      "Epoch 6/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3251 - accuracy: 0.8442\n",
      "Epoch 6/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3344 - accuracy: 0.8387\n",
      " 901/1076 [========================>.....] - ETA: 0s - loss: 0.3244 - accuracy: 0.8427Epoch 6/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3263 - accuracy: 0.8405\n",
      "Epoch 6/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3275 - accuracy: 0.8414\n",
      "Epoch 6/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3244 - accuracy: 0.8426\n",
      " 186/1076 [====>.........................] - ETA: 1s - loss: 0.3250 - accuracy: 0.8454Epoch 6/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3200 - accuracy: 0.8446\n",
      "1052/1076 [============================>.] - ETA: 0s - loss: 0.3199 - accuracy: 0.8440Epoch 7/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3201 - accuracy: 0.8437\n",
      "Epoch 7/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3316 - accuracy: 0.8397\n",
      "Epoch 7/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3204 - accuracy: 0.8458\n",
      "Epoch 7/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3219 - accuracy: 0.8448\n",
      "  25/1076 [..............................] - ETA: 2s - loss: 0.3096 - accuracy: 0.8612Epoch 7/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3187 - accuracy: 0.8465\n",
      "Epoch 7/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3158 - accuracy: 0.8478\n",
      "Epoch 8/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3141 - accuracy: 0.8501\n",
      "Epoch 8/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3274 - accuracy: 0.8424\n",
      "Epoch 8/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3157 - accuracy: 0.8475\n",
      "Epoch 8/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3174 - accuracy: 0.8468\n",
      "Epoch 8/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3147 - accuracy: 0.8464\n",
      "Epoch 8/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3117 - accuracy: 0.8504\n",
      "Epoch 9/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3100 - accuracy: 0.8528\n",
      "   1/1076 [..............................] - ETA: 2s - loss: 0.2061 - accuracy: 0.9375Epoch 9/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3213 - accuracy: 0.8486\n",
      "Epoch 9/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3119 - accuracy: 0.8505\n",
      "Epoch 9/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3143 - accuracy: 0.8489\n",
      "Epoch 9/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3103 - accuracy: 0.8514\n",
      "Epoch 9/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3077 - accuracy: 0.8516\n",
      "Epoch 10/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3066 - accuracy: 0.8528\n",
      "Epoch 10/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3196 - accuracy: 0.8499\n",
      "  28/1076 [..............................] - ETA: 2s - loss: 0.3134 - accuracy: 0.8560Epoch 10/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3081 - accuracy: 0.8532\n",
      "Epoch 10/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3114 - accuracy: 0.8514\n",
      "Epoch 10/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3073 - accuracy: 0.8517\n",
      "Epoch 10/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3045 - accuracy: 0.8552\n",
      "Epoch 11/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3028 - accuracy: 0.8562\n",
      "Epoch 11/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3165 - accuracy: 0.8513\n",
      "Epoch 11/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3049 - accuracy: 0.8535\n",
      "Epoch 11/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3082 - accuracy: 0.8531\n",
      "Epoch 11/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3033 - accuracy: 0.8545\n",
      "Epoch 11/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3007 - accuracy: 0.8573\n",
      "Epoch 12/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3016 - accuracy: 0.8550\n",
      "Epoch 12/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3115 - accuracy: 0.8524\n",
      "  50/1076 [>.............................] - ETA: 2s - loss: 0.2956 - accuracy: 0.8625Epoch 12/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3024 - accuracy: 0.8541\n",
      "Epoch 12/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3060 - accuracy: 0.8551\n",
      "Epoch 12/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3005 - accuracy: 0.8577\n",
      "Epoch 12/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2992 - accuracy: 0.8566\n",
      "Epoch 13/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2969 - accuracy: 0.8601\n",
      " 741/1076 [===================>..........] - ETA: 0s - loss: 0.2987 - accuracy: 0.8575Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3096 - accuracy: 0.8543\n",
      "Epoch 13/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3000 - accuracy: 0.8559\n",
      "Epoch 13/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3027 - accuracy: 0.8555\n",
      "Epoch 13/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2980 - accuracy: 0.8587\n",
      " 298/1076 [=======>......................] - ETA: 1s - loss: 0.2931 - accuracy: 0.8575Epoch 13/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2961 - accuracy: 0.8590\n",
      "Epoch 14/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2953 - accuracy: 0.8612\n",
      "Epoch 14/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3072 - accuracy: 0.8548\n",
      "Epoch 14/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2966 - accuracy: 0.8586\n",
      " 934/1076 [=========================>....] - ETA: 0s - loss: 0.2946 - accuracy: 0.8600Epoch 14/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3008 - accuracy: 0.8577\n",
      "Epoch 14/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2956 - accuracy: 0.8588\n",
      "Epoch 14/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2944 - accuracy: 0.8602\n",
      " 681/1076 [=================>............] - ETA: 0s - loss: 0.2943 - accuracy: 0.8589Epoch 15/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2918 - accuracy: 0.8619\n",
      "Epoch 15/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3037 - accuracy: 0.8575\n",
      " 868/1076 [=======================>......] - ETA: 0s - loss: 0.2950 - accuracy: 0.8598Epoch 15/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2948 - accuracy: 0.8591\n",
      "Epoch 15/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2979 - accuracy: 0.8578\n",
      "Epoch 15/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2942 - accuracy: 0.8599\n",
      "Epoch 15/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2926 - accuracy: 0.8598\n",
      "Epoch 16/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2905 - accuracy: 0.8620\n",
      "Epoch 16/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3034 - accuracy: 0.8593\n",
      "Epoch 16/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2923 - accuracy: 0.8603\n",
      "Epoch 16/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2975 - accuracy: 0.8589\n",
      " 297/1076 [=======>......................] - ETA: 1s - loss: 0.2844 - accuracy: 0.8654Epoch 16/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2914 - accuracy: 0.8612\n",
      "Epoch 16/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2897 - accuracy: 0.8608\n",
      "Epoch 17/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2880 - accuracy: 0.8640\n",
      "Epoch 17/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3015 - accuracy: 0.8609\n",
      "Epoch 17/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2948 - accuracy: 0.8624\n",
      "Epoch 17/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2903 - accuracy: 0.8632\n",
      "Epoch 17/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2896 - accuracy: 0.8639\n",
      " 422/1076 [==========>...................] - ETA: 1s - loss: 0.2864 - accuracy: 0.8657Epoch 17/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2888 - accuracy: 0.8627\n",
      "1052/1076 [============================>.] - ETA: 0s - loss: 0.2866 - accuracy: 0.8642Epoch 18/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2862 - accuracy: 0.8646\n",
      "Epoch 18/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2987 - accuracy: 0.8611\n",
      "Epoch 18/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2930 - accuracy: 0.8607\n",
      "Epoch 18/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2884 - accuracy: 0.8632\n",
      "Epoch 18/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2881 - accuracy: 0.8638\n",
      "Epoch 18/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2862 - accuracy: 0.8654\n",
      "Epoch 19/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2845 - accuracy: 0.8660\n",
      "Epoch 19/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2950 - accuracy: 0.8627\n",
      "Epoch 19/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2865 - accuracy: 0.8646\n",
      "Epoch 19/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2911 - accuracy: 0.8632\n",
      "  32/1076 [..............................] - ETA: 1s - loss: 0.2871 - accuracy: 0.8682Epoch 19/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2866 - accuracy: 0.8650\n",
      "Epoch 19/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2852 - accuracy: 0.8651\n",
      "Epoch 20/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2832 - accuracy: 0.8659\n",
      "Epoch 20/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2946 - accuracy: 0.8625\n",
      "  83/1076 [=>............................] - ETA: 1s - loss: 0.2834 - accuracy: 0.8671Epoch 20/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2850 - accuracy: 0.8648\n",
      "Epoch 20/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2897 - accuracy: 0.8626\n",
      "Epoch 20/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2852 - accuracy: 0.8657\n",
      "Epoch 20/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2830 - accuracy: 0.8666\n",
      "Epoch 21/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2807 - accuracy: 0.8677\n",
      "Epoch 21/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2917 - accuracy: 0.8643\n",
      "Epoch 21/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2833 - accuracy: 0.8656\n",
      "Epoch 21/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2871 - accuracy: 0.8644\n",
      "Epoch 21/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2838 - accuracy: 0.8664\n",
      "Epoch 21/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2820 - accuracy: 0.8659\n",
      "Epoch 22/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2797 - accuracy: 0.8684\n",
      "Epoch 22/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2890 - accuracy: 0.8689\n",
      "Epoch 22/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2814 - accuracy: 0.8676\n",
      "Epoch 22/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2865 - accuracy: 0.8647\n",
      "Epoch 22/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2824 - accuracy: 0.8669\n",
      "Epoch 22/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2806 - accuracy: 0.8687\n",
      " 471/1076 [============>.................] - ETA: 1s - loss: 0.2759 - accuracy: 0.8712Epoch 23/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2798 - accuracy: 0.8681\n",
      "Epoch 23/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2862 - accuracy: 0.8676\n",
      "Epoch 23/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2797 - accuracy: 0.8689\n",
      "Epoch 23/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2853 - accuracy: 0.8663\n",
      "Epoch 23/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2806 - accuracy: 0.8672\n",
      " 274/1076 [======>.......................] - ETA: 1s - loss: 0.2755 - accuracy: 0.8721Epoch 23/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2790 - accuracy: 0.8684\n",
      "Epoch 24/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2775 - accuracy: 0.8694\n",
      "Epoch 24/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2851 - accuracy: 0.8690\n",
      "Epoch 24/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2781 - accuracy: 0.8690\n",
      "Epoch 24/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2839 - accuracy: 0.8668\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2798 - accuracy: 0.8687\n",
      "Epoch 24/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2772 - accuracy: 0.8681\n",
      " 700/1076 [==================>...........] - ETA: 0s - loss: 0.2761 - accuracy: 0.8695Epoch 25/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2760 - accuracy: 0.8707\n",
      "Epoch 25/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2851 - accuracy: 0.8680\n",
      "Epoch 25/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2775 - accuracy: 0.8693\n",
      "Epoch 25/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2826 - accuracy: 0.8684\n",
      "Epoch 25/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2777 - accuracy: 0.8699\n",
      "Epoch 25/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2774 - accuracy: 0.8711\n",
      "Epoch 26/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2754 - accuracy: 0.8696\n",
      "1029/1076 [===========================>..] - ETA: 0s - loss: 0.2801 - accuracy: 0.8708Epoch 26/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2803 - accuracy: 0.8705\n",
      "Epoch 26/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2765 - accuracy: 0.8700\n",
      "Epoch 26/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2808 - accuracy: 0.8706\n",
      "Epoch 26/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2765 - accuracy: 0.8700\n",
      "Epoch 26/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2761 - accuracy: 0.8709\n",
      "Epoch 27/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2737 - accuracy: 0.8732\n",
      "Epoch 27/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2815 - accuracy: 0.8701\n",
      " 162/1076 [===>..........................] - ETA: 1s - loss: 0.2738 - accuracy: 0.8706Epoch 27/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2742 - accuracy: 0.8708\n",
      " 438/1076 [===========>..................] - ETA: 1s - loss: 0.2736 - accuracy: 0.8703Epoch 27/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2804 - accuracy: 0.8686\n",
      "Epoch 27/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2756 - accuracy: 0.8710\n",
      "Epoch 27/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2738 - accuracy: 0.8696\n",
      "Epoch 28/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2720 - accuracy: 0.8726\n",
      "Epoch 28/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2799 - accuracy: 0.8725\n",
      "Epoch 28/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2730 - accuracy: 0.8715\n",
      " 390/1076 [=========>....................] - ETA: 1s - loss: 0.2693 - accuracy: 0.8756Epoch 28/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2784 - accuracy: 0.8693\n",
      "Epoch 28/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2753 - accuracy: 0.8709\n",
      "Epoch 28/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2736 - accuracy: 0.8720\n",
      "Epoch 29/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2724 - accuracy: 0.8719\n",
      "Epoch 29/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2780 - accuracy: 0.8732\n",
      "Epoch 29/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2723 - accuracy: 0.8723\n",
      "Epoch 29/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2778 - accuracy: 0.8691\n",
      "Epoch 29/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2737 - accuracy: 0.8734\n",
      "Epoch 29/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2716 - accuracy: 0.8732\n",
      " 569/1076 [==============>...............] - ETA: 0s - loss: 0.2772 - accuracy: 0.8698Epoch 30/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2702 - accuracy: 0.8735\n",
      "Epoch 30/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2770 - accuracy: 0.8706\n",
      "Epoch 30/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2709 - accuracy: 0.8725\n",
      "Epoch 30/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2773 - accuracy: 0.8714\n",
      "Epoch 30/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2731 - accuracy: 0.8730\n",
      "Epoch 30/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2715 - accuracy: 0.8731\n",
      "Epoch 31/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2707 - accuracy: 0.8735\n",
      "Epoch 31/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2767 - accuracy: 0.8730\n",
      "Epoch 31/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2693 - accuracy: 0.8730\n",
      "Epoch 31/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2749 - accuracy: 0.8718\n",
      "Epoch 31/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2723 - accuracy: 0.8726\n",
      "Epoch 31/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2710 - accuracy: 0.8734\n",
      "Epoch 32/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2684 - accuracy: 0.8754\n",
      " 324/1076 [========>.....................] - ETA: 1s - loss: 0.2714 - accuracy: 0.8710Epoch 32/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2794 - accuracy: 0.8732\n",
      "Epoch 32/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2687 - accuracy: 0.8739\n",
      "Epoch 32/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2744 - accuracy: 0.8715\n",
      " 350/1076 [========>.....................] - ETA: 1s - loss: 0.2638 - accuracy: 0.8771Epoch 32/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2704 - accuracy: 0.8728\n",
      "Epoch 32/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2695 - accuracy: 0.8741\n",
      "Epoch 33/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2685 - accuracy: 0.8735\n",
      "Epoch 33/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2734 - accuracy: 0.8738\n",
      "Epoch 33/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2675 - accuracy: 0.8755\n",
      "Epoch 33/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2739 - accuracy: 0.8731\n",
      "  34/1076 [..............................] - ETA: 1s - loss: 0.2664 - accuracy: 0.8704Epoch 33/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2694 - accuracy: 0.8740\n",
      "Epoch 33/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2690 - accuracy: 0.8741\n",
      " 872/1076 [=======================>......] - ETA: 0s - loss: 0.2707 - accuracy: 0.8758Epoch 34/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2676 - accuracy: 0.8749\n",
      "Epoch 34/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2728 - accuracy: 0.8741\n",
      " 724/1076 [===================>..........] - ETA: 0s - loss: 0.2673 - accuracy: 0.8743Epoch 34/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2668 - accuracy: 0.8753\n",
      "Epoch 34/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2728 - accuracy: 0.8725\n",
      " 589/1076 [===============>..............] - ETA: 0s - loss: 0.2669 - accuracy: 0.8746Epoch 34/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2688 - accuracy: 0.8758\n",
      "Epoch 34/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2681 - accuracy: 0.8742\n",
      "Epoch 35/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2657 - accuracy: 0.8756\n",
      "Epoch 35/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2717 - accuracy: 0.8754\n",
      "Epoch 35/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2651 - accuracy: 0.8749\n",
      "Epoch 35/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2719 - accuracy: 0.8743\n",
      "Epoch 35/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2679 - accuracy: 0.8759\n",
      "Epoch 35/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2669 - accuracy: 0.8752\n",
      " 819/1076 [=====================>........] - ETA: 0s - loss: 0.2719 - accuracy: 0.8771Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2655 - accuracy: 0.8761\n",
      " 891/1076 [=======================>......] - ETA: 0s - loss: 0.2717 - accuracy: 0.8772Epoch 36/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2733 - accuracy: 0.8758\n",
      "Epoch 36/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2646 - accuracy: 0.8771\n",
      "Epoch 36/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2715 - accuracy: 0.8738\n",
      "  27/1076 [..............................] - ETA: 2s - loss: 0.2488 - accuracy: 0.8912Epoch 36/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2666 - accuracy: 0.8770\n",
      " 844/1076 [======================>.......] - ETA: 0s - loss: 0.2614 - accuracy: 0.8798Epoch 36/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2657 - accuracy: 0.8775\n",
      "Epoch 37/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2646 - accuracy: 0.8777\n",
      "Epoch 37/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2712 - accuracy: 0.8770\n",
      "Epoch 37/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2699 - accuracy: 0.8743\n",
      "Epoch 37/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2640 - accuracy: 0.8782\n",
      " 541/1076 [==============>...............] - ETA: 1s - loss: 0.2591 - accuracy: 0.8812Epoch 37/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2666 - accuracy: 0.8758\n",
      "Epoch 37/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2656 - accuracy: 0.8769\n",
      "Epoch 38/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2638 - accuracy: 0.8774\n",
      "Epoch 38/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2691 - accuracy: 0.8779\n",
      " 327/1076 [========>.....................] - ETA: 1s - loss: 0.2600 - accuracy: 0.8820Epoch 38/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2693 - accuracy: 0.8758\n",
      "Epoch 38/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2639 - accuracy: 0.8773\n",
      " 358/1076 [========>.....................] - ETA: 1s - loss: 0.2566 - accuracy: 0.8870Epoch 38/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2662 - accuracy: 0.8766\n",
      "1049/1076 [============================>.] - ETA: 0s - loss: 0.2648 - accuracy: 0.8765Epoch 38/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2651 - accuracy: 0.8765\n",
      "Epoch 39/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2628 - accuracy: 0.8784\n",
      "Epoch 39/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2696 - accuracy: 0.8781\n",
      "Epoch 39/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2687 - accuracy: 0.8759\n",
      "Epoch 39/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2622 - accuracy: 0.8775\n",
      "Epoch 39/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2651 - accuracy: 0.8762\n",
      "Epoch 39/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2638 - accuracy: 0.8785\n",
      " 988/1076 [==========================>...] - ETA: 0s - loss: 0.2612 - accuracy: 0.8785Epoch 40/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2620 - accuracy: 0.8781\n",
      "Epoch 40/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2693 - accuracy: 0.8768\n",
      "Epoch 40/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2672 - accuracy: 0.8772\n",
      "Epoch 40/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2618 - accuracy: 0.8775\n",
      "Epoch 40/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2633 - accuracy: 0.8779\n",
      "Epoch 41/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2640 - accuracy: 0.8778\n",
      "Epoch 40/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2615 - accuracy: 0.8792\n",
      "Epoch 41/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2678 - accuracy: 0.8791\n",
      " 211/1076 [====>.........................] - ETA: 1s - loss: 0.2415 - accuracy: 0.8911Epoch 41/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2663 - accuracy: 0.8777\n",
      "Epoch 41/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2604 - accuracy: 0.8797\n",
      "Epoch 41/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2610 - accuracy: 0.8798\n",
      "Epoch 42/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2634 - accuracy: 0.8779\n",
      "Epoch 41/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2608 - accuracy: 0.8792\n",
      "Epoch 42/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2663 - accuracy: 0.8796\n",
      "Epoch 42/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2666 - accuracy: 0.8759\n",
      " 599/1076 [===============>..............] - ETA: 0s - loss: 0.2560 - accuracy: 0.8797Epoch 42/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2599 - accuracy: 0.8784\n",
      "Epoch 42/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2612 - accuracy: 0.8794 loss: 0.2624 - accuracy: \n",
      "Epoch 43/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2629 - accuracy: 0.8793\n",
      "Epoch 42/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2606 - accuracy: 0.8782\n",
      "Epoch 43/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2655 - accuracy: 0.8793\n",
      "Epoch 43/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2653 - accuracy: 0.8772\n",
      "Epoch 43/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2592 - accuracy: 0.8804\n",
      "Epoch 43/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2604 - accuracy: 0.8789\n",
      "Epoch 44/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2596 - accuracy: 0.8802\n",
      "Epoch 44/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2615 - accuracy: 0.8799\n",
      " 149/1076 [===>..........................] - ETA: 1s - loss: 0.2638 - accuracy: 0.8756Epoch 43/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2656 - accuracy: 0.8819\n",
      "Epoch 44/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2648 - accuracy: 0.8773\n",
      "Epoch 44/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2586 - accuracy: 0.8798\n",
      "Epoch 44/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2602 - accuracy: 0.8784\n",
      "Epoch 45/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2579 - accuracy: 0.8812\n",
      "Epoch 45/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2614 - accuracy: 0.8795\n",
      "Epoch 44/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2636 - accuracy: 0.8799\n",
      "Epoch 45/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2633 - accuracy: 0.8793\n",
      "Epoch 45/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2578 - accuracy: 0.8791\n",
      "Epoch 45/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2602 - accuracy: 0.8789\n",
      "Epoch 46/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2578 - accuracy: 0.8804\n",
      "Epoch 46/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2597 - accuracy: 0.8795\n",
      "Epoch 45/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2630 - accuracy: 0.8807\n",
      "Epoch 46/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2635 - accuracy: 0.8792\n",
      "Epoch 46/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2572 - accuracy: 0.8807\n",
      "Epoch 46/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2594 - accuracy: 0.8800\n",
      " 976/1076 [==========================>...] - ETA: 0s - loss: 0.2569 - accuracy: 0.8812Epoch 47/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2571 - accuracy: 0.8816\n",
      "Epoch 47/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2598 - accuracy: 0.8796\n",
      " 211/1076 [====>.........................] - ETA: 1s - loss: 0.2613 - accuracy: 0.8806Epoch 46/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2631 - accuracy: 0.8806\n",
      "Epoch 47/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2616 - accuracy: 0.8795\n",
      "Epoch 47/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2569 - accuracy: 0.8809\n",
      "Epoch 47/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2576 - accuracy: 0.8804\n",
      "Epoch 48/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2570 - accuracy: 0.8812\n",
      "Epoch 48/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2585 - accuracy: 0.8796\n",
      "Epoch 47/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2592 - accuracy: 0.8817\n",
      "Epoch 48/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2609 - accuracy: 0.8791\n",
      "Epoch 48/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2563 - accuracy: 0.8801\n",
      "Epoch 48/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2585 - accuracy: 0.8794\n",
      "Epoch 49/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2552 - accuracy: 0.8818\n",
      "Epoch 49/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2584 - accuracy: 0.8810\n",
      "Epoch 48/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2620 - accuracy: 0.8795\n",
      "Epoch 49/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2612 - accuracy: 0.8800\n",
      "Epoch 49/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2550 - accuracy: 0.8819\n",
      " 586/1076 [===============>..............] - ETA: 0s - loss: 0.2554 - accuracy: 0.8851Epoch 49/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2560 - accuracy: 0.8826\n",
      "Epoch 50/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2557 - accuracy: 0.8809\n",
      "Epoch 50/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2583 - accuracy: 0.8811\n",
      "Epoch 49/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2614 - accuracy: 0.8815\n",
      "Epoch 50/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2605 - accuracy: 0.8790\n",
      "Epoch 50/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2544 - accuracy: 0.8829\n",
      "Epoch 50/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2562 - accuracy: 0.8823\n",
      " 704/1076 [==================>...........] - ETA: 0s - loss: 0.2517 - accuracy: 0.8853Epoch 51/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2543 - accuracy: 0.8823\n",
      "Epoch 51/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2578 - accuracy: 0.8814\n",
      "Epoch 50/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2566 - accuracy: 0.8827\n",
      "Epoch 51/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2591 - accuracy: 0.8813\n",
      "Epoch 51/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2540 - accuracy: 0.8829\n",
      "Epoch 51/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2554 - accuracy: 0.8813\n",
      "Epoch 52/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2549 - accuracy: 0.8827\n",
      "Epoch 52/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2566 - accuracy: 0.8814\n",
      "Epoch 51/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2582 - accuracy: 0.8824\n",
      "Epoch 52/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2595 - accuracy: 0.8805\n",
      "Epoch 52/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2528 - accuracy: 0.8823\n",
      "Epoch 52/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2549 - accuracy: 0.8828\n",
      "Epoch 53/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2533 - accuracy: 0.8841\n",
      "Epoch 53/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2564 - accuracy: 0.8816\n",
      "Epoch 52/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2675 - accuracy: 0.8814\n",
      "Epoch 53/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2586 - accuracy: 0.8800\n",
      " 981/1076 [==========================>...] - ETA: 0s - loss: 0.2525 - accuracy: 0.8838Epoch 53/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2525 - accuracy: 0.8841\n",
      "Epoch 53/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2541 - accuracy: 0.8828\n",
      " 760/1076 [====================>.........] - ETA: 0s - loss: 0.2532 - accuracy: 0.8832Epoch 54/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2536 - accuracy: 0.8823\n",
      "Epoch 54/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2561 - accuracy: 0.8812\n",
      "Epoch 53/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2584 - accuracy: 0.8814\n",
      "Epoch 54/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2574 - accuracy: 0.8812\n",
      "Epoch 54/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2517 - accuracy: 0.8843\n",
      "Epoch 54/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2543 - accuracy: 0.8818\n",
      "Epoch 55/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2522 - accuracy: 0.8842\n",
      "Epoch 55/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2555 - accuracy: 0.8830\n",
      "Epoch 54/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2625 - accuracy: 0.8807\n",
      "Epoch 55/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2569 - accuracy: 0.8821\n",
      "Epoch 55/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2512 - accuracy: 0.8853\n",
      "Epoch 55/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2533 - accuracy: 0.8835\n",
      " 662/1076 [=================>............] - ETA: 0s - loss: 0.2547 - accuracy: 0.8834Epoch 56/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2519 - accuracy: 0.8843\n",
      "Epoch 56/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2543 - accuracy: 0.8827\n",
      "Epoch 55/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2543 - accuracy: 0.8839\n",
      "Epoch 56/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2561 - accuracy: 0.8827\n",
      "Epoch 56/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2514 - accuracy: 0.8852\n",
      "Epoch 56/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2536 - accuracy: 0.8825\n",
      "Epoch 57/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2515 - accuracy: 0.8837\n",
      "Epoch 57/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2540 - accuracy: 0.8816\n",
      "Epoch 56/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2541 - accuracy: 0.8851\n",
      "Epoch 57/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2552 - accuracy: 0.8835\n",
      "Epoch 57/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2499 - accuracy: 0.8845\n",
      " 923/1076 [========================>.....] - ETA: 0s - loss: 0.2489 - accuracy: 0.8850Epoch 57/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2518 - accuracy: 0.8833\n",
      "Epoch 58/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2509 - accuracy: 0.8834\n",
      "Epoch 58/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2551 - accuracy: 0.8842\n",
      "Epoch 58/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2536 - accuracy: 0.8835\n",
      "Epoch 57/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2560 - accuracy: 0.8818\n",
      "Epoch 58/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2517 - accuracy: 0.8836\n",
      "Epoch 59/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2495 - accuracy: 0.8841\n",
      "Epoch 58/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2509 - accuracy: 0.8860\n",
      "Epoch 59/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2558 - accuracy: 0.8829\n",
      "Epoch 59/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2533 - accuracy: 0.8829\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2547 - accuracy: 0.8821\n",
      "Epoch 59/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2503 - accuracy: 0.8857\n",
      " 937/1076 [=========================>....] - ETA: 0s - loss: 0.2488 - accuracy: 0.8849Epoch 60/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2489 - accuracy: 0.8847\n",
      "Epoch 59/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2488 - accuracy: 0.8847\n",
      "Epoch 60/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2532 - accuracy: 0.8841\n",
      " 351/1076 [========>.....................] - ETA: 1s - loss: 0.2468 - accuracy: 0.8869Epoch 60/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2528 - accuracy: 0.8841\n",
      "Epoch 59/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2545 - accuracy: 0.8838\n",
      "Epoch 60/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2516 - accuracy: 0.8841\n",
      "Epoch 61/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2478 - accuracy: 0.8863\n",
      "Epoch 60/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2492 - accuracy: 0.8854\n",
      "Epoch 61/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2553 - accuracy: 0.8829\n",
      "Epoch 61/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2517 - accuracy: 0.8841\n",
      "Epoch 60/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2537 - accuracy: 0.8840\n",
      "Epoch 61/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2499 - accuracy: 0.8842\n",
      " 529/1076 [=============>................] - ETA: 1s - loss: 0.2481 - accuracy: 0.8866Epoch 62/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2473 - accuracy: 0.8855\n",
      "Epoch 61/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2485 - accuracy: 0.8850\n",
      "Epoch 62/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2522 - accuracy: 0.8861\n",
      "Epoch 62/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2516 - accuracy: 0.8841\n",
      " 498/1076 [============>.................] - ETA: 1s - loss: 0.2440 - accuracy: 0.8885Epoch 61/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2526 - accuracy: 0.8844\n",
      "Epoch 62/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2493 - accuracy: 0.8852\n",
      "Epoch 63/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2474 - accuracy: 0.8860\n",
      "  76/1076 [=>............................] - ETA: 2s - loss: 0.2549 - accuracy: 0.8779Epoch 62/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2492 - accuracy: 0.8849\n",
      " 134/1076 [==>...........................] - ETA: 1s - loss: 0.2413 - accuracy: 0.8885Epoch 63/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2517 - accuracy: 0.8856\n",
      "Epoch 63/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2508 - accuracy: 0.8838\n",
      "Epoch 62/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2492 - accuracy: 0.8851\n",
      "Epoch 64/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2520 - accuracy: 0.8847\n",
      "Epoch 63/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2455 - accuracy: 0.8875\n",
      "Epoch 63/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2485 - accuracy: 0.8852\n",
      "Epoch 64/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2673 - accuracy: 0.8824\n",
      "Epoch 64/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2508 - accuracy: 0.8858\n",
      "Epoch 63/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2481 - accuracy: 0.8861\n",
      "Epoch 65/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2521 - accuracy: 0.8845\n",
      "Epoch 64/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2475 - accuracy: 0.8849\n",
      "Epoch 65/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2457 - accuracy: 0.8884\n",
      "Epoch 64/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2521 - accuracy: 0.8851\n",
      "Epoch 65/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2502 - accuracy: 0.8846\n",
      "Epoch 64/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2481 - accuracy: 0.8860\n",
      "Epoch 66/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2517 - accuracy: 0.8845\n",
      " 489/1076 [============>.................] - ETA: 1s - loss: 0.2466 - accuracy: 0.8880Epoch 65/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2474 - accuracy: 0.8846\n",
      "Epoch 66/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2454 - accuracy: 0.8870\n",
      "Epoch 65/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2479 - accuracy: 0.8876\n",
      " 195/1076 [====>.........................] - ETA: 1s - loss: 0.2341 - accuracy: 0.8901Epoch 66/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2488 - accuracy: 0.8857\n",
      "Epoch 65/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2479 - accuracy: 0.8836\n",
      "Epoch 67/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2508 - accuracy: 0.8849\n",
      "Epoch 66/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2461 - accuracy: 0.8879\n",
      "Epoch 67/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2447 - accuracy: 0.8869\n",
      "Epoch 66/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2541 - accuracy: 0.8853\n",
      "Epoch 67/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2495 - accuracy: 0.8860\n",
      " 653/1076 [=================>............] - ETA: 1s - loss: 0.2418 - accuracy: 0.8898Epoch 66/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2469 - accuracy: 0.8865\n",
      "Epoch 68/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2495 - accuracy: 0.8868\n",
      "Epoch 67/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2467 - accuracy: 0.8863\n",
      "Epoch 68/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2450 - accuracy: 0.8879\n",
      "Epoch 67/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2457 - accuracy: 0.8867\n",
      "Epoch 68/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2484 - accuracy: 0.8874\n",
      " 674/1076 [=================>............] - ETA: 0s - loss: 0.2412 - accuracy: 0.8900Epoch 67/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2473 - accuracy: 0.8855\n",
      "Epoch 69/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2509 - accuracy: 0.8851\n",
      "Epoch 68/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2452 - accuracy: 0.8861\n",
      "Epoch 69/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2434 - accuracy: 0.8869\n",
      "Epoch 68/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2463 - accuracy: 0.8879\n",
      "Epoch 69/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2488 - accuracy: 0.8863\n",
      "Epoch 68/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2462 - accuracy: 0.8880\n",
      "Epoch 70/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2495 - accuracy: 0.8861\n",
      "Epoch 69/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2454 - accuracy: 0.8878\n",
      "Epoch 70/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2437 - accuracy: 0.8882\n",
      "Epoch 69/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2475 - accuracy: 0.8873\n",
      "Epoch 70/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2483 - accuracy: 0.8869\n",
      "Epoch 69/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2446 - accuracy: 0.8871\n",
      "Epoch 71/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2493 - accuracy: 0.8869\n",
      "Epoch 70/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2449 - accuracy: 0.8865\n",
      "Epoch 71/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2433 - accuracy: 0.8875\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2557 - accuracy: 0.8864\n",
      " 375/1076 [=========>....................] - ETA: 1s - loss: 0.2477 - accuracy: 0.8840Epoch 71/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2471 - accuracy: 0.8864\n",
      "Epoch 70/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2467 - accuracy: 0.8879\n",
      "Epoch 72/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2494 - accuracy: 0.8841\n",
      "Epoch 71/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2444 - accuracy: 0.8861\n",
      "Epoch 72/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2425 - accuracy: 0.8884\n",
      "Epoch 71/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2622 - accuracy: 0.8856\n",
      "Epoch 72/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2466 - accuracy: 0.8857\n",
      "Epoch 71/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2448 - accuracy: 0.8873\n",
      "Epoch 73/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2456 - accuracy: 0.8873\n",
      "Epoch 73/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2476 - accuracy: 0.8883\n",
      "Epoch 72/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2420 - accuracy: 0.8878\n",
      "Epoch 72/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2473 - accuracy: 0.8864\n",
      "Epoch 73/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2458 - accuracy: 0.8874\n",
      "Epoch 72/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2442 - accuracy: 0.8882\n",
      "Epoch 74/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2440 - accuracy: 0.8880\n",
      "Epoch 74/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2483 - accuracy: 0.8863\n",
      "Epoch 73/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2418 - accuracy: 0.8883\n",
      "Epoch 73/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2424 - accuracy: 0.8889\n",
      "Epoch 74/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2463 - accuracy: 0.8884\n",
      "Epoch 73/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2445 - accuracy: 0.8877\n",
      "Epoch 75/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2431 - accuracy: 0.8884\n",
      "Epoch 75/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2467 - accuracy: 0.8867\n",
      "Epoch 74/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2406 - accuracy: 0.8895\n",
      "Epoch 74/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2453 - accuracy: 0.8890\n",
      "Epoch 75/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2465 - accuracy: 0.8871\n",
      " 224/1076 [=====>........................] - ETA: 1s - loss: 0.2421 - accuracy: 0.8930Epoch 74/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2445 - accuracy: 0.8888\n",
      "Epoch 76/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2428 - accuracy: 0.8890\n",
      " 630/1076 [================>.............] - ETA: 0s - loss: 0.2395 - accuracy: 0.8913Epoch 76/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2470 - accuracy: 0.8888\n",
      "Epoch 75/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2410 - accuracy: 0.8882\n",
      "Epoch 75/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2473 - accuracy: 0.8882\n",
      " 837/1076 [======================>.......] - ETA: 0s - loss: 0.2458 - accuracy: 0.8874Epoch 76/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2458 - accuracy: 0.8875\n",
      "Epoch 75/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2441 - accuracy: 0.8892\n",
      "Epoch 77/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2424 - accuracy: 0.8892\n",
      "Epoch 77/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2469 - accuracy: 0.8876\n",
      " 985/1076 [==========================>...] - ETA: 0s - loss: 0.2412 - accuracy: 0.8895Epoch 76/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2399 - accuracy: 0.8901\n",
      "Epoch 76/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2470 - accuracy: 0.8866\n",
      "Epoch 77/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2458 - accuracy: 0.8883\n",
      " 471/1076 [============>.................] - ETA: 1s - loss: 0.2324 - accuracy: 0.8944Epoch 76/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2430 - accuracy: 0.8885\n",
      "Epoch 78/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2417 - accuracy: 0.8895\n",
      "Epoch 78/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2463 - accuracy: 0.8871\n",
      "Epoch 77/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2392 - accuracy: 0.8888\n",
      "Epoch 77/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2489 - accuracy: 0.8875\n",
      "Epoch 78/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2444 - accuracy: 0.8883\n",
      "Epoch 77/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2409 - accuracy: 0.8902\n",
      "Epoch 79/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2418 - accuracy: 0.8886\n",
      "Epoch 79/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2453 - accuracy: 0.8893\n",
      " 797/1076 [=====================>........] - ETA: 0s - loss: 0.2400 - accuracy: 0.8897Epoch 78/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2395 - accuracy: 0.8897\n",
      "Epoch 78/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2412 - accuracy: 0.8891\n",
      "Epoch 79/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2434 - accuracy: 0.8902\n",
      "Epoch 78/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2418 - accuracy: 0.8893\n",
      "Epoch 80/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2401 - accuracy: 0.8902\n",
      "Epoch 80/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2445 - accuracy: 0.8893\n",
      " 517/1076 [=============>................] - ETA: 1s - loss: 0.2413 - accuracy: 0.8912Epoch 79/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2379 - accuracy: 0.8908\n",
      "Epoch 79/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2432 - accuracy: 0.8884\n",
      "Epoch 80/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2436 - accuracy: 0.8888\n",
      "Epoch 79/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2419 - accuracy: 0.8899\n",
      "Epoch 81/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2411 - accuracy: 0.8894\n",
      "Epoch 81/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2456 - accuracy: 0.8883\n",
      "Epoch 80/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2380 - accuracy: 0.8898\n",
      "Epoch 80/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2480 - accuracy: 0.8872\n",
      "Epoch 81/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2424 - accuracy: 0.8885\n",
      "Epoch 80/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2413 - accuracy: 0.8907\n",
      "Epoch 82/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2410 - accuracy: 0.8882\n",
      "Epoch 82/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2446 - accuracy: 0.8889\n",
      "Epoch 81/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2377 - accuracy: 0.8923\n",
      "Epoch 81/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2448 - accuracy: 0.8884\n",
      "Epoch 82/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2402 - accuracy: 0.8901\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2424 - accuracy: 0.8899\n",
      "Epoch 83/100\n",
      "Epoch 81/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2404 - accuracy: 0.8881\n",
      "Epoch 83/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2446 - accuracy: 0.8890\n",
      "Epoch 82/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2375 - accuracy: 0.8904\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2483 - accuracy: 0.8877\n",
      "Epoch 83/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2405 - accuracy: 0.8903\n",
      "Epoch 84/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2431 - accuracy: 0.8897\n",
      "Epoch 82/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2393 - accuracy: 0.8897\n",
      "Epoch 84/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2382 - accuracy: 0.8902\n",
      "Epoch 83/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2434 - accuracy: 0.8901\n",
      "Epoch 83/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2479 - accuracy: 0.8893\n",
      "Epoch 84/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2406 - accuracy: 0.8912\n",
      "Epoch 85/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2418 - accuracy: 0.8895\n",
      "Epoch 83/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2392 - accuracy: 0.8899\n",
      "Epoch 85/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2365 - accuracy: 0.8923\n",
      "Epoch 84/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2441 - accuracy: 0.8888\n",
      " 509/1076 [=============>................] - ETA: 1s - loss: 0.2359 - accuracy: 0.8923Epoch 84/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2394 - accuracy: 0.8899\n",
      "Epoch 85/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2397 - accuracy: 0.8916\n",
      "Epoch 86/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2413 - accuracy: 0.8899\n",
      "Epoch 84/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2392 - accuracy: 0.8905\n",
      " 721/1076 [===================>..........] - ETA: 0s - loss: 0.2347 - accuracy: 0.8905Epoch 86/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2366 - accuracy: 0.8893\n",
      "Epoch 85/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2416 - accuracy: 0.8907\n",
      "Epoch 85/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2470 - accuracy: 0.8900\n",
      "Epoch 86/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2396 - accuracy: 0.8904\n",
      "Epoch 87/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2408 - accuracy: 0.8906\n",
      "Epoch 85/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2384 - accuracy: 0.8902\n",
      "  48/1076 [>.............................] - ETA: 2s - loss: 0.2013 - accuracy: 0.9089Epoch 87/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2367 - accuracy: 0.8912\n",
      "Epoch 86/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2424 - accuracy: 0.8905\n",
      " 510/1076 [=============>................] - ETA: 1s - loss: 0.2284 - accuracy: 0.8966Epoch 86/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2456 - accuracy: 0.8901\n",
      " 699/1076 [==================>...........] - ETA: 0s - loss: 0.2308 - accuracy: 0.8935Epoch 87/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2378 - accuracy: 0.8896\n",
      "Epoch 88/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2414 - accuracy: 0.8903\n",
      "Epoch 86/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2385 - accuracy: 0.8928\n",
      "Epoch 88/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2358 - accuracy: 0.8925\n",
      "Epoch 87/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2418 - accuracy: 0.8899\n",
      "Epoch 87/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2426 - accuracy: 0.8893\n",
      "Epoch 88/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2384 - accuracy: 0.8921\n",
      "Epoch 89/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2404 - accuracy: 0.8905\n",
      "Epoch 87/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2385 - accuracy: 0.8914\n",
      "Epoch 89/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2351 - accuracy: 0.8920\n",
      "Epoch 88/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2412 - accuracy: 0.8904\n",
      " 513/1076 [=============>................] - ETA: 1s - loss: 0.2386 - accuracy: 0.8900Epoch 88/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2390 - accuracy: 0.8902\n",
      "Epoch 89/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2382 - accuracy: 0.8913\n",
      "Epoch 90/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2399 - accuracy: 0.8916\n",
      "Epoch 88/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2379 - accuracy: 0.8896\n",
      "Epoch 90/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2353 - accuracy: 0.8911\n",
      " 456/1076 [===========>..................] - ETA: 1s - loss: 0.2367 - accuracy: 0.8898Epoch 89/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2415 - accuracy: 0.8905\n",
      "Epoch 89/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2474 - accuracy: 0.8871\n",
      "Epoch 90/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2382 - accuracy: 0.8912\n",
      "Epoch 91/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2389 - accuracy: 0.8917\n",
      "Epoch 89/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2371 - accuracy: 0.8920\n",
      "Epoch 91/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2345 - accuracy: 0.8929\n",
      "Epoch 90/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2385 - accuracy: 0.8905\n",
      "Epoch 91/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2417 - accuracy: 0.8896\n",
      "   1/1076 [..............................] - ETA: 2s - loss: 0.1777 - accuracy: 0.9062Epoch 90/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2377 - accuracy: 0.8913\n",
      "Epoch 92/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2394 - accuracy: 0.8917\n",
      "Epoch 90/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2377 - accuracy: 0.8914\n",
      "Epoch 92/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2338 - accuracy: 0.8931\n",
      "Epoch 91/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2460 - accuracy: 0.8896\n",
      "Epoch 92/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2405 - accuracy: 0.8914\n",
      "Epoch 91/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2380 - accuracy: 0.8907\n",
      "Epoch 93/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2363 - accuracy: 0.8913\n",
      "Epoch 93/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2396 - accuracy: 0.8899\n",
      "Epoch 91/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2339 - accuracy: 0.8925\n",
      "Epoch 92/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2476 - accuracy: 0.8902\n",
      "Epoch 93/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2404 - accuracy: 0.8924\n",
      "Epoch 92/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2377 - accuracy: 0.8903\n",
      "Epoch 94/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2362 - accuracy: 0.8914\n",
      "Epoch 94/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2391 - accuracy: 0.8912\n",
      "Epoch 92/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2334 - accuracy: 0.8926\n",
      "Epoch 93/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2381 - accuracy: 0.8907\n",
      " 457/1076 [===========>..................] - ETA: 1s - loss: 0.2358 - accuracy: 0.8921Epoch 94/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2392 - accuracy: 0.8913\n",
      "Epoch 93/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2372 - accuracy: 0.8924\n",
      "Epoch 95/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2353 - accuracy: 0.8937\n",
      "Epoch 95/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2382 - accuracy: 0.8918\n",
      "Epoch 93/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2328 - accuracy: 0.8946\n",
      "Epoch 94/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2366 - accuracy: 0.8911\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2400 - accuracy: 0.8909\n",
      "Epoch 94/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2365 - accuracy: 0.8922\n",
      "Epoch 96/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2351 - accuracy: 0.8939\n",
      "Epoch 96/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2385 - accuracy: 0.8913\n",
      " 588/1076 [===============>..............] - ETA: 1s - loss: 0.2366 - accuracy: 0.8932Epoch 94/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2338 - accuracy: 0.8932\n",
      "Epoch 95/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2396 - accuracy: 0.8911\n",
      "Epoch 96/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2393 - accuracy: 0.8918\n",
      "Epoch 95/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2360 - accuracy: 0.8913\n",
      "Epoch 97/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2348 - accuracy: 0.8932\n",
      "Epoch 97/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2383 - accuracy: 0.8906\n",
      "Epoch 95/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2332 - accuracy: 0.8926\n",
      "Epoch 96/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2385 - accuracy: 0.8926\n",
      "Epoch 97/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2386 - accuracy: 0.8931\n",
      "Epoch 96/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2371 - accuracy: 0.8915\n",
      "Epoch 98/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2344 - accuracy: 0.8933\n",
      "Epoch 98/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2383 - accuracy: 0.8905\n",
      "Epoch 96/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2331 - accuracy: 0.8936\n",
      "Epoch 97/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2382 - accuracy: 0.8909\n",
      "Epoch 98/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2394 - accuracy: 0.8912\n",
      "Epoch 97/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2358 - accuracy: 0.8928\n",
      "Epoch 99/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2336 - accuracy: 0.8934\n",
      "Epoch 99/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2373 - accuracy: 0.8920\n",
      " 682/1076 [==================>...........] - ETA: 0s - loss: 0.2296 - accuracy: 0.8936Epoch 97/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2317 - accuracy: 0.8931\n",
      "Epoch 98/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2399 - accuracy: 0.8924\n",
      "Epoch 99/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2381 - accuracy: 0.8917\n",
      "Epoch 98/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2354 - accuracy: 0.8918\n",
      "Epoch 100/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2344 - accuracy: 0.8935\n",
      "Epoch 100/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2369 - accuracy: 0.8935\n",
      " 620/1076 [================>.............] - ETA: 0s - loss: 0.2355 - accuracy: 0.8931Epoch 98/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2324 - accuracy: 0.8937\n",
      "Epoch 99/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2391 - accuracy: 0.8925\n",
      "Epoch 100/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2381 - accuracy: 0.8916\n",
      "  78/1076 [=>............................] - ETA: 1s - loss: 0.2122 - accuracy: 0.9006Epoch 99/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2355 - accuracy: 0.8915\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2342 - accuracy: 0.8915\n",
      "269/269 [==============================] - 1s 2ms/step - loss: 0.3900 - accuracy: 0.8349\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2374 - accuracy: 0.8910\n",
      " 709/1076 [==================>...........] - ETA: 0s - loss: 0.2322 - accuracy: 0.8940Epoch 99/100\n",
      " 164/1076 [===>..........................] - ETA: 1s - loss: 0.2250 - accuracy: 0.8973Epoch 1/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2354 - accuracy: 0.8932\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2316 - accuracy: 0.8942\n",
      "Epoch 100/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2376 - accuracy: 0.8909\n",
      "Epoch 100/100\n",
      "269/269 [==============================] - 1s 1ms/step - loss: 0.4391 - accuracy: 0.8452\n",
      " 448/1076 [===========>..................] - ETA: 0s - loss: 0.4080 - accuracy: 0.7980Epoch 1/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2366 - accuracy: 0.8927\n",
      "Epoch 100/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2320 - accuracy: 0.8931\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2385 - accuracy: 0.8905\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.4128 - accuracy: 0.8018\n",
      "Epoch 2/100\n",
      "269/269 [==============================] - 1s 1ms/step - loss: 0.3833 - accuracy: 0.8408\n",
      "269/269 [==============================] - 1s 1ms/step - loss: 0.3692 - accuracy: 0.8410\n",
      "1008/1076 [===========================>..] - ETA: 0s - loss: 0.2353 - accuracy: 0.8935Epoch 1/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2367 - accuracy: 0.8926\n",
      " 586/1076 [===============>..............] - ETA: 0s - loss: 0.3568 - accuracy: 0.8256Epoch 1/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 0.3962 - accuracy: 0.8047\n",
      "Epoch 2/100\n",
      "269/269 [==============================] - 0s 1ms/step - loss: 0.4773 - accuracy: 0.8331\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 0.3558 - accuracy: 0.8268\n",
      " 158/1076 [===>..........................] - ETA: 1s - loss: 0.4446 - accuracy: 0.7822Epoch 3/100\n",
      " 326/1076 [========>.....................] - ETA: 1s - loss: 0.4316 - accuracy: 0.7815Epoch 1/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3573 - accuracy: 0.8252\n",
      "Epoch 3/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.4050 - accuracy: 0.8013\n",
      "Epoch 2/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.4025 - accuracy: 0.8058\n",
      "  85/1076 [=>............................] - ETA: 1s - loss: 0.3776 - accuracy: 0.8099Epoch 2/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3425 - accuracy: 0.8319\n",
      "Epoch 4/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.8988 - accuracy: 0.7060\n",
      "Epoch 2/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3487 - accuracy: 0.8296\n",
      "1037/1076 [===========================>..] - ETA: 0s - loss: 0.3606 - accuracy: 0.8236Epoch 4/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3592 - accuracy: 0.8242\n",
      "Epoch 3/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3586 - accuracy: 0.8240\n",
      "Epoch 3/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3361 - accuracy: 0.8345\n",
      " 293/1076 [=======>......................] - ETA: 1s - loss: 0.3356 - accuracy: 0.8375Epoch 5/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6437 - accuracy: 0.6861\n",
      "Epoch 3/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3407 - accuracy: 0.8345\n",
      "Epoch 5/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3431 - accuracy: 0.8327\n",
      "Epoch 4/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3503 - accuracy: 0.8285\n",
      "Epoch 4/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3308 - accuracy: 0.8402\n",
      "Epoch 6/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6055 - accuracy: 0.6848\n",
      "Epoch 4/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3351 - accuracy: 0.8385\n",
      "Epoch 6/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3370 - accuracy: 0.8349\n",
      "Epoch 5/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3429 - accuracy: 0.8326\n",
      "Epoch 5/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3266 - accuracy: 0.8421\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5801 - accuracy: 0.7175\n",
      "Epoch 5/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3272 - accuracy: 0.8418\n",
      "Epoch 7/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3310 - accuracy: 0.8421\n",
      "Epoch 6/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3355 - accuracy: 0.8362\n",
      "Epoch 6/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3230 - accuracy: 0.8462\n",
      "Epoch 8/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5350 - accuracy: 0.7590\n",
      "Epoch 6/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3237 - accuracy: 0.8427\n",
      " 256/1076 [======>.......................] - ETA: 1s - loss: 0.5170 - accuracy: 0.7660Epoch 8/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3273 - accuracy: 0.8417\n",
      "Epoch 7/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3291 - accuracy: 0.8421\n",
      "Epoch 7/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3178 - accuracy: 0.8489\n",
      "Epoch 9/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5159 - accuracy: 0.7619\n",
      "Epoch 7/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3195 - accuracy: 0.8470\n",
      "Epoch 9/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3209 - accuracy: 0.8444\n",
      "Epoch 8/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3238 - accuracy: 0.8452\n",
      "Epoch 8/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3154 - accuracy: 0.8496\n",
      " 547/1076 [==============>...............] - ETA: 0s - loss: 0.5277 - accuracy: 0.7664Epoch 10/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5624 - accuracy: 0.7701\n",
      "Epoch 8/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3171 - accuracy: 0.8471\n",
      "Epoch 10/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3168 - accuracy: 0.8465\n",
      "Epoch 9/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3221 - accuracy: 0.8467\n",
      " 868/1076 [=======================>......] - ETA: 0s - loss: 0.3099 - accuracy: 0.8512Epoch 9/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3111 - accuracy: 0.8501\n",
      "Epoch 11/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5724 - accuracy: 0.7691\n",
      "Epoch 9/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3117 - accuracy: 0.8527\n",
      "Epoch 11/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3136 - accuracy: 0.8481\n",
      "Epoch 10/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3171 - accuracy: 0.8496\n",
      "Epoch 10/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3082 - accuracy: 0.8530\n",
      "Epoch 12/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.8553 - accuracy: 0.7411\n",
      "Epoch 10/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3098 - accuracy: 0.8532\n",
      "Epoch 12/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3105 - accuracy: 0.8521\n",
      "Epoch 11/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3131 - accuracy: 0.8519\n",
      "Epoch 11/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3049 - accuracy: 0.8549\n",
      "Epoch 13/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.7297 - accuracy: 0.7007\n",
      "Epoch 11/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3058 - accuracy: 0.8555\n",
      " 765/1076 [====================>.........] - ETA: 0s - loss: 0.3027 - accuracy: 0.8588Epoch 13/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3091 - accuracy: 0.8528\n",
      "Epoch 12/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3085 - accuracy: 0.8529\n",
      "Epoch 12/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3030 - accuracy: 0.8587\n",
      "Epoch 14/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.8090 - accuracy: 0.7080\n",
      "Epoch 12/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3046 - accuracy: 0.8558\n",
      "Epoch 14/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3058 - accuracy: 0.8559\n",
      "Epoch 13/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3062 - accuracy: 0.8561\n",
      "Epoch 13/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2974 - accuracy: 0.8604\n",
      "Epoch 15/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5984 - accuracy: 0.7239\n",
      "Epoch 13/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3022 - accuracy: 0.8572\n",
      "Epoch 15/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3007 - accuracy: 0.8603\n",
      "Epoch 14/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3041 - accuracy: 0.8581\n",
      "Epoch 14/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3765 - accuracy: 0.8566\n",
      "Epoch 16/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6298 - accuracy: 0.7121\n",
      "Epoch 14/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3012 - accuracy: 0.8596\n",
      "Epoch 16/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2988 - accuracy: 0.8579\n",
      "Epoch 15/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3020 - accuracy: 0.8578\n",
      "Epoch 15/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3667 - accuracy: 0.8467\n",
      "Epoch 17/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6289 - accuracy: 0.7175\n",
      "Epoch 15/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2990 - accuracy: 0.8594\n",
      "Epoch 17/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2964 - accuracy: 0.8613\n",
      "Epoch 16/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2996 - accuracy: 0.8582\n",
      "Epoch 16/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2993 - accuracy: 0.8599\n",
      "Epoch 18/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5950 - accuracy: 0.7349\n",
      "Epoch 16/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2982 - accuracy: 0.8601\n",
      "Epoch 18/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2960 - accuracy: 0.8634\n",
      "Epoch 17/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2967 - accuracy: 0.8621\n",
      "Epoch 17/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2915 - accuracy: 0.8638\n",
      "Epoch 19/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.9520 - accuracy: 0.7096\n",
      "Epoch 17/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2949 - accuracy: 0.8605\n",
      "Epoch 19/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2916 - accuracy: 0.8633\n",
      "Epoch 18/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2948 - accuracy: 0.8616\n",
      "Epoch 18/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2911 - accuracy: 0.8654\n",
      "Epoch 20/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 7.1518 - accuracy: 0.6953\n",
      "Epoch 18/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2932 - accuracy: 0.8639\n",
      "Epoch 20/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2895 - accuracy: 0.8645\n",
      "Epoch 19/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2913 - accuracy: 0.8631\n",
      "Epoch 19/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2848 - accuracy: 0.8678\n",
      " 303/1076 [=======>......................] - ETA: 1s - loss: 0.2750 - accuracy: 0.8758Epoch 21/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 2.4969 - accuracy: 0.6724\n",
      "Epoch 19/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2913 - accuracy: 0.8653\n",
      "Epoch 21/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2858 - accuracy: 0.8669\n",
      "Epoch 20/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2893 - accuracy: 0.8649\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2861 - accuracy: 0.8680\n",
      "Epoch 22/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.8169 - accuracy: 0.6794\n",
      "Epoch 20/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2891 - accuracy: 0.8659\n",
      " 919/1076 [========================>.....] - ETA: 0s - loss: 0.2889 - accuracy: 0.8652Epoch 22/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2862 - accuracy: 0.8655\n",
      "Epoch 21/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2890 - accuracy: 0.8650\n",
      "Epoch 21/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2863 - accuracy: 0.8684\n",
      "Epoch 23/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6014 - accuracy: 0.6762\n",
      "Epoch 21/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2883 - accuracy: 0.8670\n",
      "Epoch 23/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2848 - accuracy: 0.8668\n",
      "Epoch 22/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2895 - accuracy: 0.8652\n",
      "Epoch 22/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2853 - accuracy: 0.8682\n",
      " 284/1076 [======>.......................] - ETA: 1s - loss: 0.2751 - accuracy: 0.8693Epoch 24/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5719 - accuracy: 0.6731\n",
      "Epoch 22/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2861 - accuracy: 0.8646\n",
      " 912/1076 [========================>.....] - ETA: 0s - loss: 0.2823 - accuracy: 0.8674Epoch 24/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2832 - accuracy: 0.8680\n",
      "Epoch 23/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2848 - accuracy: 0.8668\n",
      "Epoch 23/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2827 - accuracy: 0.8687\n",
      "Epoch 25/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5315 - accuracy: 0.6788\n",
      "Epoch 23/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2875 - accuracy: 0.8664\n",
      "Epoch 25/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2813 - accuracy: 0.8702\n",
      "Epoch 24/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2852 - accuracy: 0.8674\n",
      "Epoch 24/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2821 - accuracy: 0.8702\n",
      "Epoch 26/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5480 - accuracy: 0.6734\n",
      "Epoch 24/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2843 - accuracy: 0.8681\n",
      "1056/1076 [============================>.] - ETA: 0s - loss: 0.2854 - accuracy: 0.8677Epoch 26/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2857 - accuracy: 0.8675\n",
      "Epoch 25/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2831 - accuracy: 0.8677\n",
      "Epoch 25/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2810 - accuracy: 0.8715\n",
      "Epoch 27/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.7315 - accuracy: 0.6734\n",
      "Epoch 25/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5466 - accuracy: 0.8603\n",
      "Epoch 27/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2769 - accuracy: 0.8710\n",
      "Epoch 26/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3592 - accuracy: 0.8638\n",
      "Epoch 26/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2812 - accuracy: 0.8721\n",
      "Epoch 28/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5969 - accuracy: 0.6771\n",
      "1000/1076 [==========================>...] - ETA: 0s - loss: 0.3280 - accuracy: 0.8538Epoch 26/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3258 - accuracy: 0.8544\n",
      "Epoch 28/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2765 - accuracy: 0.8718\n",
      "Epoch 27/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.3203 - accuracy: 0.8593\n",
      "Epoch 27/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2783 - accuracy: 0.8729\n",
      "Epoch 29/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.4835 - accuracy: 0.6762\n",
      "Epoch 27/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2918 - accuracy: 0.8645\n",
      "Epoch 29/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2764 - accuracy: 0.8713\n",
      "Epoch 28/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2785 - accuracy: 0.8714\n",
      "Epoch 28/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2760 - accuracy: 0.8725\n",
      " 262/1076 [======>.......................] - ETA: 1s - loss: 0.2657 - accuracy: 0.8761Epoch 30/100\n",
      "269/269 [==============================] - 1s 2ms/step - loss: 0.3785 - accuracy: 0.8420\n",
      " 697/1076 [==================>...........] - ETA: 0s - loss: 0.2720 - accuracy: 0.8734Epoch 1/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.4989 - accuracy: 0.6751\n",
      "Epoch 28/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2845 - accuracy: 0.8680\n",
      "Epoch 30/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2752 - accuracy: 0.8711\n",
      "Epoch 29/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2760 - accuracy: 0.8721\n",
      "Epoch 29/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2750 - accuracy: 0.8730\n",
      "Epoch 31/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2793 - accuracy: 0.8689\n",
      "1074/1076 [============================>.] - ETA: 0s - loss: 0.5168 - accuracy: 0.6716Epoch 31/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5167 - accuracy: 0.6717\n",
      "Epoch 29/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2755 - accuracy: 0.8718\n",
      "Epoch 30/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2740 - accuracy: 0.8734\n",
      "Epoch 30/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 2.7982 - accuracy: 0.7119\n",
      "Epoch 2/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2760 - accuracy: 0.8728\n",
      "Epoch 32/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2777 - accuracy: 0.8715\n",
      "Epoch 32/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.4898 - accuracy: 0.6783\n",
      "Epoch 30/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2709 - accuracy: 0.8756\n",
      "Epoch 31/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5381 - accuracy: 0.7227\n",
      "Epoch 3/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2735 - accuracy: 0.8742\n",
      "Epoch 31/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2726 - accuracy: 0.8743\n",
      "Epoch 33/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2761 - accuracy: 0.8723\n",
      " 902/1076 [========================>.....] - ETA: 0s - loss: 0.4872 - accuracy: 0.7692Epoch 33/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2706 - accuracy: 0.8749\n",
      "Epoch 32/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.5749 - accuracy: 0.6771\n",
      " 928/1076 [========================>.....] - ETA: 0s - loss: 0.4877 - accuracy: 0.7685Epoch 31/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.4827 - accuracy: 0.7710\n",
      "Epoch 4/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2729 - accuracy: 0.8722\n",
      "Epoch 32/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2694 - accuracy: 0.8761\n",
      "Epoch 34/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2746 - accuracy: 0.8728\n",
      " 779/1076 [====================>.........] - ETA: 0s - loss: 0.2696 - accuracy: 0.8759Epoch 34/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2707 - accuracy: 0.8760\n",
      "Epoch 33/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.3373 - accuracy: 0.6715\n",
      "Epoch 32/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.4522 - accuracy: 0.7800\n",
      " 127/1076 [==>...........................] - ETA: 1s - loss: 1.0175 - accuracy: 0.6683Epoch 5/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2713 - accuracy: 0.8742\n",
      " 972/1076 [==========================>...] - ETA: 0s - loss: 0.2726 - accuracy: 0.8752Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2727 - accuracy: 0.8749\n",
      "Epoch 35/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2740 - accuracy: 0.8719\n",
      "Epoch 35/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2681 - accuracy: 0.8752\n",
      "Epoch 34/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.7682 - accuracy: 0.6691\n",
      "Epoch 33/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.4439 - accuracy: 0.7830\n",
      "Epoch 6/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2704 - accuracy: 0.8732\n",
      "Epoch 34/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2709 - accuracy: 0.8749\n",
      "Epoch 36/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2766 - accuracy: 0.8730\n",
      "Epoch 36/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2690 - accuracy: 0.8759\n",
      "Epoch 35/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6906 - accuracy: 0.6741\n",
      "Epoch 34/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.4617 - accuracy: 0.7758\n",
      "Epoch 7/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2728 - accuracy: 0.8741\n",
      "Epoch 35/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2683 - accuracy: 0.8782\n",
      "Epoch 37/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2736 - accuracy: 0.8762\n",
      "Epoch 37/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2662 - accuracy: 0.8764\n",
      "Epoch 36/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6941 - accuracy: 0.6714\n",
      "Epoch 35/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5049 - accuracy: 0.7726\n",
      "Epoch 8/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2699 - accuracy: 0.8753\n",
      "Epoch 36/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2673 - accuracy: 0.8777\n",
      "Epoch 38/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2721 - accuracy: 0.8752\n",
      "Epoch 38/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6336 - accuracy: 0.6716\n",
      "Epoch 36/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2651 - accuracy: 0.8755\n",
      "Epoch 37/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.0163 - accuracy: 0.7401\n",
      "Epoch 9/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2677 - accuracy: 0.8763\n",
      "Epoch 37/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2782 - accuracy: 0.8752\n",
      " 262/1076 [======>.......................] - ETA: 1s - loss: 0.2660 - accuracy: 0.8794Epoch 39/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2697 - accuracy: 0.8748\n",
      " 867/1076 [=======================>......] - ETA: 0s - loss: 0.2678 - accuracy: 0.8740Epoch 39/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.7001 - accuracy: 0.6732\n",
      "Epoch 37/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2677 - accuracy: 0.8755\n",
      "Epoch 38/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.8461 - accuracy: 0.7332\n",
      "Epoch 10/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2681 - accuracy: 0.8750\n",
      "   1/1076 [..............................] - ETA: 2s - loss: 0.2973 - accuracy: 0.9375Epoch 38/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2613 - accuracy: 0.8789\n",
      " 251/1076 [=====>........................] - ETA: 1s - loss: 0.2573 - accuracy: 0.8853Epoch 40/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2712 - accuracy: 0.8772\n",
      "Epoch 40/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2668 - accuracy: 0.8773\n",
      "Epoch 39/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.1379 - accuracy: 0.6735\n",
      "Epoch 38/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5352 - accuracy: 0.7482\n",
      "Epoch 11/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2639 - accuracy: 0.8795\n",
      "Epoch 39/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2625 - accuracy: 0.8809\n",
      "Epoch 41/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2687 - accuracy: 0.8754\n",
      "Epoch 41/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2692 - accuracy: 0.8754\n",
      "Epoch 40/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.8656 - accuracy: 0.6765\n",
      "Epoch 39/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5172 - accuracy: 0.7594\n",
      "Epoch 12/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2663 - accuracy: 0.8792\n",
      "1072/1076 [============================>.] - ETA: 0s - loss: 0.2622 - accuracy: 0.8788Epoch 40/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2622 - accuracy: 0.8787\n",
      "  23/1076 [..............................] - ETA: 2s - loss: 0.6515 - accuracy: 0.7337Epoch 42/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2723 - accuracy: 0.8778\n",
      "Epoch 42/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2626 - accuracy: 0.8771\n",
      "Epoch 41/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6914 - accuracy: 0.6715\n",
      "Epoch 40/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.8168 - accuracy: 0.7205\n",
      "Epoch 13/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2664 - accuracy: 0.8769\n",
      "Epoch 41/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2631 - accuracy: 0.8796\n",
      "Epoch 43/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2672 - accuracy: 0.8777\n",
      "Epoch 43/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2611 - accuracy: 0.8795\n",
      "Epoch 42/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5840 - accuracy: 0.6758\n",
      "Epoch 41/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.9535 - accuracy: 0.7189\n",
      "Epoch 14/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2607 - accuracy: 0.8800\n",
      "Epoch 44/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2683 - accuracy: 0.8780\n",
      "Epoch 42/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2607 - accuracy: 0.8804\n",
      "Epoch 44/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2580 - accuracy: 0.8787\n",
      "Epoch 43/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5233 - accuracy: 0.6760\n",
      "Epoch 42/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 6.4516 - accuracy: 0.6736\n",
      "Epoch 15/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2644 - accuracy: 0.8788\n",
      "Epoch 45/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2635 - accuracy: 0.8787\n",
      "Epoch 43/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2617 - accuracy: 0.8788\n",
      "Epoch 45/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2595 - accuracy: 0.8790\n",
      "Epoch 44/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5039 - accuracy: 0.6719\n",
      "Epoch 43/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.7367 - accuracy: 0.6682\n",
      "Epoch 16/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2579 - accuracy: 0.8817\n",
      "Epoch 46/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2617 - accuracy: 0.8809\n",
      "Epoch 44/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2623 - accuracy: 0.8790\n",
      "Epoch 46/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2637 - accuracy: 0.8794\n",
      " 985/1076 [==========================>...] - ETA: 0s - loss: 0.5270 - accuracy: 0.6740Epoch 45/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5231 - accuracy: 0.6734\n",
      "Epoch 17/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2578 - accuracy: 0.8815\n",
      "Epoch 47/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.7657 - accuracy: 0.6749\n",
      " 151/1076 [===>..........................] - ETA: 1s - loss: 0.2397 - accuracy: 0.8949Epoch 44/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2636 - accuracy: 0.8805\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2609 - accuracy: 0.8807\n",
      "Epoch 47/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2589 - accuracy: 0.8809\n",
      "  52/1076 [>.............................] - ETA: 2s - loss: 0.2490 - accuracy: 0.8792Epoch 46/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2595 - accuracy: 0.8813\n",
      "Epoch 48/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5594 - accuracy: 0.6755\n",
      "Epoch 18/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 10.0510 - accuracy: 0.6760\n",
      "Epoch 45/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2599 - accuracy: 0.8800\n",
      "Epoch 46/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2615 - accuracy: 0.8796\n",
      "Epoch 48/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2580 - accuracy: 0.8814\n",
      "Epoch 47/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5499 - accuracy: 0.6688\n",
      "Epoch 19/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2539 - accuracy: 0.8820\n",
      "   1/1076 [..............................] - ETA: 2s - loss: 0.4322 - accuracy: 0.7188Epoch 49/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2593 - accuracy: 0.8812\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.2134 - accuracy: 0.6764\n",
      "Epoch 47/100\n",
      "Epoch 46/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2618 - accuracy: 0.8797\n",
      "Epoch 49/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2577 - accuracy: 0.8806\n",
      "Epoch 48/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2572 - accuracy: 0.8837\n",
      " 998/1076 [==========================>...] - ETA: 0s - loss: 0.6459 - accuracy: 0.6726Epoch 50/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6014 - accuracy: 0.6699\n",
      "Epoch 20/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2649 - accuracy: 0.8813\n",
      "Epoch 48/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6550 - accuracy: 0.6731\n",
      "Epoch 47/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2581 - accuracy: 0.8805\n",
      "Epoch 50/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2580 - accuracy: 0.8801\n",
      "Epoch 49/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2558 - accuracy: 0.8841\n",
      "Epoch 51/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.7851 - accuracy: 0.6751\n",
      "   1/1076 [..............................] - ETA: 2s - loss: 0.2053 - accuracy: 0.9062Epoch 21/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2581 - accuracy: 0.8831\n",
      "Epoch 49/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5450 - accuracy: 0.6726\n",
      "Epoch 48/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2588 - accuracy: 0.8820\n",
      " 987/1076 [==========================>...] - ETA: 0s - loss: 0.2544 - accuracy: 0.8838Epoch 51/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2537 - accuracy: 0.8828\n",
      "Epoch 50/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2546 - accuracy: 0.8836\n",
      "Epoch 52/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6789 - accuracy: 0.6742\n",
      "Epoch 22/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2575 - accuracy: 0.8826\n",
      "Epoch 50/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5260 - accuracy: 0.6749\n",
      "Epoch 49/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2576 - accuracy: 0.8819\n",
      "Epoch 52/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2573 - accuracy: 0.8803\n",
      "Epoch 51/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2575 - accuracy: 0.8841\n",
      "Epoch 53/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.5515 - accuracy: 0.6724\n",
      "Epoch 23/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2591 - accuracy: 0.8827\n",
      "Epoch 51/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.5286 - accuracy: 0.6729\n",
      "Epoch 50/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2619 - accuracy: 0.8809\n",
      "1027/1076 [===========================>..] - ETA: 0s - loss: 0.2643 - accuracy: 0.8804Epoch 53/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5309 - accuracy: 0.6725\n",
      "Epoch 24/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2635 - accuracy: 0.8801\n",
      "Epoch 52/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2588 - accuracy: 0.8833\n",
      "Epoch 54/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2548 - accuracy: 0.8819\n",
      "Epoch 52/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.5511 - accuracy: 0.6756\n",
      "Epoch 51/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2561 - accuracy: 0.8843==============>..........] - ETA: 0s - loss: 0.2549 - accuracy: \n",
      "Epoch 54/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.4907 - accuracy: 0.6753\n",
      " 923/1076 [========================>.....] - ETA: 0s - loss: 0.2549 - accuracy: 0.8840Epoch 25/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2521 - accuracy: 0.8858\n",
      "Epoch 53/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2506 - accuracy: 0.8847\n",
      "Epoch 55/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2558 - accuracy: 0.8835\n",
      "Epoch 53/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6244 - accuracy: 0.6742\n",
      " 163/1076 [===>..........................] - ETA: 2s - loss: 0.2346 - accuracy: 0.8924Epoch 52/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2547 - accuracy: 0.8839\n",
      "Epoch 55/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5473 - accuracy: 0.6703\n",
      "Epoch 26/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2506 - accuracy: 0.8834\n",
      "Epoch 54/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2495 - accuracy: 0.8837\n",
      "Epoch 56/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2565 - accuracy: 0.8849\n",
      "Epoch 54/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.0121 - accuracy: 0.6713\n",
      "Epoch 53/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2584 - accuracy: 0.8850\n",
      "Epoch 56/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2526 - accuracy: 0.8826\n",
      "Epoch 55/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5146 - accuracy: 0.6757\n",
      "Epoch 27/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2505 - accuracy: 0.8875\n",
      " 991/1076 [==========================>...] - ETA: 0s - loss: 0.2569 - accuracy: 0.8835Epoch 57/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2581 - accuracy: 0.8826\n",
      "Epoch 55/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.9104 - accuracy: 0.6765\n",
      "Epoch 54/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2492 - accuracy: 0.8847\n",
      "Epoch 56/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2556 - accuracy: 0.8823\n",
      "Epoch 57/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.4870 - accuracy: 0.6784\n",
      "Epoch 28/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2508 - accuracy: 0.8859\n",
      " 933/1076 [=========================>....] - ETA: 0s - loss: 0.6491 - accuracy: 0.6765Epoch 58/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2530 - accuracy: 0.8840\n",
      "Epoch 56/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6719 - accuracy: 0.6735\n",
      "Epoch 55/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2550 - accuracy: 0.8828\n",
      "Epoch 57/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2515 - accuracy: 0.8835\n",
      "   1/1076 [..............................] - ETA: 2s - loss: 0.2650 - accuracy: 0.8750Epoch 58/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5081 - accuracy: 0.6697\n",
      "  25/1076 [..............................] - ETA: 2s - loss: 0.2368 - accuracy: 0.8900Epoch 29/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2480 - accuracy: 0.8872\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2537 - accuracy: 0.8834\n",
      "Epoch 57/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.8042 - accuracy: 0.6715\n",
      "Epoch 56/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2657 - accuracy: 0.8822\n",
      "Epoch 58/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2532 - accuracy: 0.8828\n",
      "Epoch 59/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.5590 - accuracy: 0.6716\n",
      "  28/1076 [..............................] - ETA: 1s - loss: 0.2112 - accuracy: 0.9118Epoch 30/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2475 - accuracy: 0.8874\n",
      "Epoch 60/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2548 - accuracy: 0.8840\n",
      "Epoch 58/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.7008 - accuracy: 0.6720\n",
      " 271/1076 [======>.......................] - ETA: 1s - loss: 0.2379 - accuracy: 0.8885Epoch 57/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2524 - accuracy: 0.8848\n",
      "Epoch 60/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2474 - accuracy: 0.8859\n",
      "Epoch 59/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.4075 - accuracy: 0.6721\n",
      "Epoch 31/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2466 - accuracy: 0.8886\n",
      "  50/1076 [>.............................] - ETA: 2s - loss: 0.9450 - accuracy: 0.6881Epoch 61/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2525 - accuracy: 0.8853\n",
      " 173/1076 [===>..........................] - ETA: 1s - loss: 0.8482 - accuracy: 0.6765Epoch 59/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.8954 - accuracy: 0.6681\n",
      "Epoch 58/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2456 - accuracy: 0.8863\n",
      "Epoch 60/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2556 - accuracy: 0.8840\n",
      "Epoch 61/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.8879 - accuracy: 0.6745\n",
      "Epoch 32/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2680 - accuracy: 0.8845\n",
      "Epoch 62/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2539 - accuracy: 0.8852\n",
      "Epoch 60/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 1.1581 - accuracy: 0.6675\n",
      "Epoch 59/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6712 - accuracy: 0.6721\n",
      "Epoch 33/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2496 - accuracy: 0.8844\n",
      "Epoch 61/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2513 - accuracy: 0.8849\n",
      "Epoch 62/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2418 - accuracy: 0.8907\n",
      "Epoch 63/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2550 - accuracy: 0.8851\n",
      "Epoch 61/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6612 - accuracy: 0.6698\n",
      "Epoch 60/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6079 - accuracy: 0.6700\n",
      "Epoch 34/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2482 - accuracy: 0.8864\n",
      "Epoch 62/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2520 - accuracy: 0.8846\n",
      "Epoch 63/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2432 - accuracy: 0.8895\n",
      "Epoch 64/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2471 - accuracy: 0.8865\n",
      "Epoch 62/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.1436 - accuracy: 0.6655\n",
      " 314/1076 [=======>......................] - ETA: 1s - loss: 0.5140 - accuracy: 0.6822Epoch 61/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 27.0884 - accuracy: 0.6717\n",
      "Epoch 35/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2455 - accuracy: 0.8868\n",
      "Epoch 63/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2501 - accuracy: 0.8861\n",
      "Epoch 64/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2424 - accuracy: 0.8889\n",
      "Epoch 65/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2499 - accuracy: 0.8849\n",
      "Epoch 63/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.8174 - accuracy: 0.6652\n",
      "Epoch 62/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.5193 - accuracy: 0.6629\n",
      "Epoch 36/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2468 - accuracy: 0.8864\n",
      "Epoch 64/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2633 - accuracy: 0.8845\n",
      "Epoch 65/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2447 - accuracy: 0.8893\n",
      "Epoch 66/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2489 - accuracy: 0.8870\n",
      "Epoch 64/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6806 - accuracy: 0.6727\n",
      "Epoch 63/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6787 - accuracy: 0.6612\n",
      "Epoch 37/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2513 - accuracy: 0.8858\n",
      "Epoch 65/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2452 - accuracy: 0.8886\n",
      "Epoch 67/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2500 - accuracy: 0.8867\n",
      "   1/1076 [..............................] - ETA: 1s - loss: 0.1496 - accuracy: 0.9375Epoch 66/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2497 - accuracy: 0.8872\n",
      "Epoch 65/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.8724 - accuracy: 0.6664\n",
      "Epoch 64/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5422 - accuracy: 0.6635\n",
      " 727/1076 [===================>..........] - ETA: 0s - loss: 1.1478 - accuracy: 0.6705Epoch 38/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2463 - accuracy: 0.8863\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2500 - accuracy: 0.8874\n",
      "Epoch 66/100\n",
      "Epoch 68/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2454 - accuracy: 0.8859\n",
      "  27/1076 [..............................] - ETA: 2s - loss: 0.4690 - accuracy: 0.6678Epoch 67/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2521 - accuracy: 0.8862\n",
      "Epoch 66/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.0885 - accuracy: 0.6665\n",
      "Epoch 65/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.4859 - accuracy: 0.6651\n",
      "Epoch 39/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2399 - accuracy: 0.8914\n",
      "Epoch 69/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2498 - accuracy: 0.8867\n",
      " 725/1076 [===================>..........] - ETA: 0s - loss: 0.7253 - accuracy: 0.6667Epoch 67/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2476 - accuracy: 0.8851\n",
      "Epoch 68/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2512 - accuracy: 0.8852\n",
      "Epoch 67/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.7945 - accuracy: 0.6682\n",
      "Epoch 66/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5164 - accuracy: 0.6649\n",
      "Epoch 40/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2399 - accuracy: 0.8915\n",
      "Epoch 70/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2475 - accuracy: 0.8864\n",
      "Epoch 68/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2449 - accuracy: 0.8870\n",
      "  26/1076 [..............................] - ETA: 2s - loss: 0.4767 - accuracy: 0.6743Epoch 69/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2469 - accuracy: 0.8869\n",
      "Epoch 68/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6681 - accuracy: 0.6671............] - ETA: 2s - loss: 0.2328 - accuracy: 0.89\n",
      "Epoch 67/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6432 - accuracy: 0.6661\n",
      "Epoch 41/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2417 - accuracy: 0.8908\n",
      "Epoch 71/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2452 - accuracy: 0.8872\n",
      "Epoch 69/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2474 - accuracy: 0.8874\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 951/1076 [=========================>....] - ETA: 0s - loss: 0.5128 - accuracy: 0.66: 0s - loss: 0.2452 - accuracy: 0.8863\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  68/1076 [>.............................] - ETA: 2s - loss: 0.2211 - accuracy: 0.9072\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  44/1076 [>.............................] - ETA: 2s - loss: 0.2354 - accuracy: 0.8885\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  92/1076 [=>............................] - ETA: 2s - loss: 0.6318 - accuracy: 0.6698\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  69/1076 [>.............................] - ETA: 2s - loss: 0.2311 - accuracy: 0.8931\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 773/1076 [====================>.........] - ETA: 0s - loss: 0.5116 - accuracy: 0.6649\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 973/1076 [==========================>...] - ETA: 0s - loss: 0.2464 - accuracy: 0.8857\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  90/1076 [=>............................] - ETA: 2s - loss: 0.2349 - accuracy: 0.9000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  68/1076 [>.............................] - ETA: 2s - loss: 0.2342 - accuracy: 0.8892\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 115/1076 [==>...........................] - ETA: 2s - loss: 0.7487 - accuracy: 0.6674\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  92/1076 [=>............................] - ETA: 2s - loss: 0.2332 - accuracy: 0.8947\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 797/1076 [=====================>........] - ETA: 0s - loss: 0.5191 - accuracy: 0.6647\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 997/1076 [==========================>...] - ETA: 0s - loss: 0.2467 - accuracy: 0.8852\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 116/1076 [==>...........................] - ETA: 2s - loss: 0.2290 - accuracy: 0.8992\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  95/1076 [=>............................] - ETA: 2s - loss: 0.2350 - accuracy: 0.8914\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 143/1076 [==>...........................] - ETA: 2s - loss: 0.7373 - accuracy: 0.6641\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 119/1076 [==>...........................] - ETA: 2s - loss: 0.2298 - accuracy: 0.8929\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 823/1076 [=====================>........] - ETA: 0s - loss: 0.5176 - accuracy: 0.6652\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1023/1076 [===========================>..] - ETA: 0s - loss: 0.2471 - accuracy: 0.8852\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 143/1076 [==>...........................] - ETA: 2s - loss: 0.2276 - accuracy: 0.8990\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 122/1076 [==>...........................] - ETA: 2s - loss: 0.2364 - accuracy: 0.8922\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 170/1076 [===>..........................] - ETA: 1s - loss: 0.6945 - accuracy: 0.6649\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 144/1076 [===>..........................] - ETA: 1s - loss: 0.2304 - accuracy: 0.8926\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 848/1076 [======================>.......] - ETA: 0s - loss: 0.5162 - accuracy: 0.6658\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1048/1076 [============================>.] - ETA: 0s - loss: 0.2465 - accuracy: 0.8855\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 167/1076 [===>..........................] - ETA: 1s - loss: 0.2288 - accuracy: 0.8971\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 145/1076 [===>..........................] - ETA: 1s - loss: 0.2352 - accuracy: 0.8931\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 193/1076 [====>.........................] - ETA: 1s - loss: 0.6775 - accuracy: 0.6622\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 168/1076 [===>..........................] - ETA: 1s - loss: 0.2323 - accuracy: 0.8930\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 871/1076 [=======================>......] - ETA: 0s - loss: 0.5219 - accuracy: 0.6652\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1070/1076 [============================>.] - ETA: 0s - loss: 0.2467 - accuracy: 0.8853\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 190/1076 [====>.........................] - ETA: 1s - loss: 0.2295 - accuracy: 0.8961\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2466 - accuracy: 0.8853\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5708 - accuracy: 0.6649\n",
      " 195/1076 [====>.........................] - ETA: 1s - loss: 0.2268 - accuracy: 0.8979Epoch 68/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.7244 - accuracy: 0.6636\n",
      "Epoch 42/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2444 - accuracy: 0.8893\n",
      "Epoch 72/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2487 - accuracy: 0.8864\n",
      "Epoch 71/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2504 - accuracy: 0.8855\n",
      "Epoch 70/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2452 - accuracy: 0.8880\n",
      "Epoch 70/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6671 - accuracy: 0.6645\n",
      " 415/1076 [==========>...................] - ETA: 1s - loss: 0.2396 - accuracy: 0.8930Epoch 69/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2466 - accuracy: 0.8869\n",
      "1044/1076 [============================>.] - ETA: 0s - loss: 0.2422 - accuracy: 0.8904Epoch 72/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.9126 - accuracy: 0.6636\n",
      "Epoch 43/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2420 - accuracy: 0.8907\n",
      "Epoch 73/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2466 - accuracy: 0.8875\n",
      "Epoch 71/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2465 - accuracy: 0.8885\n",
      "Epoch 71/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.0486 - accuracy: 0.6668\n",
      "Epoch 70/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.7656 - accuracy: 0.6674\n",
      "Epoch 44/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2462 - accuracy: 0.8871\n",
      "Epoch 73/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2449 - accuracy: 0.8900\n",
      "Epoch 74/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2476 - accuracy: 0.8881\n",
      "Epoch 72/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2501 - accuracy: 0.8881\n",
      "Epoch 72/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6655 - accuracy: 0.6654\n",
      "Epoch 71/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6232 - accuracy: 0.6656\n",
      "Epoch 45/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2478 - accuracy: 0.8869\n",
      "  29/1076 [..............................] - ETA: 1s - loss: 2.1534 - accuracy: 0.6767Epoch 74/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2417 - accuracy: 0.8902\n",
      "  27/1076 [..............................] - ETA: 2s - loss: 0.2089 - accuracy: 0.8993Epoch 73/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2386 - accuracy: 0.8921\n",
      "Epoch 75/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2470 - accuracy: 0.8868\n",
      "Epoch 73/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.1065 - accuracy: 0.6626\n",
      " 475/1076 [============>.................] - ETA: 1s - loss: 0.2327 - accuracy: 0.8918Epoch 72/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2446 - accuracy: 0.8882\n",
      "Epoch 75/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2446 - accuracy: 0.8912\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.8264 - accuracy: 0.6658\n",
      "Epoch 76/100\n",
      " 589/1076 [===============>..............] - ETA: 0s - loss: 1.5400 - accuracy: 0.6645Epoch 46/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2388 - accuracy: 0.8904\n",
      "  27/1076 [..............................] - ETA: 2s - loss: 0.2317 - accuracy: 0.8854Epoch 74/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2414 - accuracy: 0.8905\n",
      "Epoch 74/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.4421 - accuracy: 0.6675\n",
      "Epoch 73/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2473 - accuracy: 0.8885\n",
      "Epoch 76/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2373 - accuracy: 0.8931\n",
      "1004/1076 [==========================>...] - ETA: 0s - loss: 0.2434 - accuracy: 0.8872Epoch 77/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2445 - accuracy: 0.8870\n",
      "Epoch 75/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5794 - accuracy: 0.6644\n",
      "Epoch 47/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2468 - accuracy: 0.8869\n",
      "Epoch 75/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.0932 - accuracy: 0.6667\n",
      "Epoch 74/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2494 - accuracy: 0.8872\n",
      "Epoch 77/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2377 - accuracy: 0.8919\n",
      "Epoch 78/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2385 - accuracy: 0.8893\n",
      " 632/1076 [================>.............] - ETA: 1s - loss: 1.0364 - accuracy: 0.6666Epoch 76/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5082 - accuracy: 0.6645\n",
      "Epoch 48/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2418 - accuracy: 0.8893\n",
      "Epoch 76/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 1.1836 - accuracy: 0.6673\n",
      " 528/1076 [=============>................] - ETA: 1s - loss: 0.2291 - accuracy: 0.8964Epoch 75/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2409 - accuracy: 0.8897\n",
      " 983/1076 [==========================>...] - ETA: 0s - loss: 0.2396 - accuracy: 0.8892Epoch 78/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2402 - accuracy: 0.8918\n",
      "Epoch 79/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.5451 - accuracy: 0.6695\n",
      "Epoch 49/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2413 - accuracy: 0.8882\n",
      "Epoch 77/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2424 - accuracy: 0.8890\n",
      "Epoch 77/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.1418 - accuracy: 0.6633\n",
      "Epoch 76/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2400 - accuracy: 0.8918\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2406 - accuracy: 0.8893\n",
      " 777/1076 [====================>.........] - ETA: 0s - loss: 0.2434 - accuracy: 0.8886Epoch 80/100\n",
      "Epoch 79/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.9653 - accuracy: 0.6685\n",
      "Epoch 50/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2436 - accuracy: 0.8881\n",
      "Epoch 78/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2448 - accuracy: 0.8892\n",
      "Epoch 78/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 6.8926 - accuracy: 0.6650\n",
      "Epoch 77/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2362 - accuracy: 0.8930\n",
      "Epoch 81/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2503 - accuracy: 0.8882\n",
      "Epoch 80/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.8816 - accuracy: 0.6685\n",
      "Epoch 51/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2402 - accuracy: 0.8897\n",
      "Epoch 79/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2471 - accuracy: 0.8878\n",
      "Epoch 79/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.0378 - accuracy: 0.6586\n",
      "Epoch 78/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2399 - accuracy: 0.8935\n",
      "Epoch 82/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2476 - accuracy: 0.8907\n",
      "Epoch 81/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.7616 - accuracy: 0.6640\n",
      "Epoch 52/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2405 - accuracy: 0.8897\n",
      "Epoch 80/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2494 - accuracy: 0.8882\n",
      "Epoch 80/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6645 - accuracy: 0.6649\n",
      "Epoch 79/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2392 - accuracy: 0.8931\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2406 - accuracy: 0.8893\n",
      "Epoch 82/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 6.4764 - accuracy: 0.6647\n",
      "Epoch 53/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2369 - accuracy: 0.8911\n",
      "Epoch 81/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2504 - accuracy: 0.8879\n",
      "Epoch 81/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6038 - accuracy: 0.6614\n",
      "Epoch 80/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2363 - accuracy: 0.8933\n",
      "Epoch 84/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2412 - accuracy: 0.8890\n",
      "Epoch 83/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.6823 - accuracy: 0.6608\n",
      "Epoch 54/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2505 - accuracy: 0.8872\n",
      "Epoch 82/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2418 - accuracy: 0.8886\n",
      " 258/1076 [======>.......................] - ETA: 1s - loss: 0.5850 - accuracy: 0.6593Epoch 82/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5495 - accuracy: 0.6639\n",
      " 586/1076 [===============>..............] - ETA: 1s - loss: 0.2474 - accuracy: 0.8890Epoch 81/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2556 - accuracy: 0.8915\n",
      "Epoch 85/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2409 - accuracy: 0.8887\n",
      " 996/1076 [==========================>...] - ETA: 0s - loss: 0.2538 - accuracy: 0.8873Epoch 84/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6341 - accuracy: 0.6620\n",
      "Epoch 55/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2525 - accuracy: 0.8876\n",
      "Epoch 83/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2383 - accuracy: 0.8913\n",
      "Epoch 83/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6398 - accuracy: 0.6625\n",
      "Epoch 82/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2391 - accuracy: 0.8933\n",
      " 933/1076 [=========================>....] - ETA: 0s - loss: 0.2384 - accuracy: 0.8914Epoch 86/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2391 - accuracy: 0.8913\n",
      "Epoch 85/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5833 - accuracy: 0.6623\n",
      "Epoch 56/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2393 - accuracy: 0.8902\n",
      "Epoch 84/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2464 - accuracy: 0.8889\n",
      " 466/1076 [===========>..................] - ETA: 1s - loss: 0.2209 - accuracy: 0.8963Epoch 84/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5655 - accuracy: 0.6627\n",
      " 601/1076 [===============>..............] - ETA: 0s - loss: 0.2346 - accuracy: 0.8930Epoch 83/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2316 - accuracy: 0.8947\n",
      "Epoch 87/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5599 - accuracy: 0.6604\n",
      "Epoch 57/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2396 - accuracy: 0.8893\n",
      "Epoch 86/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2364 - accuracy: 0.8908\n",
      " 231/1076 [=====>........................] - ETA: 1s - loss: 0.2366 - accuracy: 0.8962Epoch 85/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2405 - accuracy: 0.8923\n",
      "Epoch 85/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6718 - accuracy: 0.6594\n",
      "Epoch 84/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2334 - accuracy: 0.8959\n",
      " 915/1076 [========================>.....] - ETA: 0s - loss: 0.2358 - accuracy: 0.8918Epoch 88/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5945 - accuracy: 0.6581\n",
      "Epoch 58/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2380 - accuracy: 0.8906\n",
      " 997/1076 [==========================>...] - ETA: 0s - loss: 0.2337 - accuracy: 0.8913Epoch 87/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2333 - accuracy: 0.8915\n",
      "Epoch 86/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2472 - accuracy: 0.8905\n",
      "Epoch 86/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6360 - accuracy: 0.6592\n",
      " 872/1076 [=======================>......] - ETA: 0s - loss: 0.2334 - accuracy: 0.8966Epoch 85/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2344 - accuracy: 0.8959\n",
      "Epoch 89/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.7169 - accuracy: 0.6596\n",
      "Epoch 59/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2402 - accuracy: 0.8907\n",
      "Epoch 88/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2398 - accuracy: 0.8904\n",
      "Epoch 87/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2348 - accuracy: 0.8928\n",
      "Epoch 87/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.7647 - accuracy: 0.6608\n",
      "Epoch 86/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2355 - accuracy: 0.8939\n",
      " 898/1076 [========================>.....] - ETA: 0s - loss: 0.2519 - accuracy: 0.8885Epoch 90/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6460 - accuracy: 0.6633\n",
      "Epoch 60/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2498 - accuracy: 0.8891\n",
      "Epoch 89/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2362 - accuracy: 0.8927\n",
      "Epoch 88/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2382 - accuracy: 0.8904\n",
      "Epoch 88/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.7078 - accuracy: 0.6631\n",
      "Epoch 87/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2373 - accuracy: 0.8934\n",
      "Epoch 91/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.7214 - accuracy: 0.6633\n",
      "Epoch 61/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2404 - accuracy: 0.8923\n",
      "Epoch 90/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2389 - accuracy: 0.8913\n",
      "Epoch 89/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2381 - accuracy: 0.8916\n",
      "Epoch 89/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6772 - accuracy: 0.6596\n",
      "Epoch 88/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2336 - accuracy: 0.8949\n",
      "Epoch 92/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.8465 - accuracy: 0.6598\n",
      "Epoch 62/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2407 - accuracy: 0.8913\n",
      "Epoch 91/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2412 - accuracy: 0.8926\n",
      "Epoch 90/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2386 - accuracy: 0.8892\n",
      "Epoch 90/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.6666 - accuracy: 0.6574\n",
      "Epoch 89/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2349 - accuracy: 0.8956\n",
      "Epoch 93/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.8777 - accuracy: 0.6658\n",
      "Epoch 63/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2358 - accuracy: 0.8930\n",
      "Epoch 92/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.2358 - accuracy: 0.8922\n",
      " 403/1076 [==========>...................] - ETA: 1s - loss: 0.6452 - accuracy: 0.6721Epoch 91/100\n",
      "1076/1076 [==============================] - 3s 3ms/step - loss: 0.2411 - accuracy: 0.8896\n",
      "Epoch 91/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 1.1997 - accuracy: 0.6618\n",
      "Epoch 90/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2342 - accuracy: 0.8941\n",
      " 565/1076 [==============>...............] - ETA: 1s - loss: 0.2330 - accuracy: 0.8920Epoch 94/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.7099 - accuracy: 0.6610\n",
      "Epoch 64/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2390 - accuracy: 0.8920\n",
      "Epoch 93/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2314 - accuracy: 0.8931\n",
      "Epoch 92/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2405 - accuracy: 0.8893\n",
      "Epoch 92/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.6533 - accuracy: 0.6628\n",
      "Epoch 91/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2403 - accuracy: 0.8941\n",
      "Epoch 95/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.7515 - accuracy: 0.6588\n",
      "Epoch 65/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2346 - accuracy: 0.8914\n",
      "Epoch 94/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2371 - accuracy: 0.8913\n",
      "Epoch 93/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2341 - accuracy: 0.8938\n",
      "Epoch 93/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.1480 - accuracy: 0.6619\n",
      "Epoch 92/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2348 - accuracy: 0.8952\n",
      "Epoch 96/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.0866 - accuracy: 0.6633\n",
      "Epoch 66/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2323 - accuracy: 0.8925\n",
      "Epoch 95/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2413 - accuracy: 0.8914\n",
      "Epoch 94/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2366 - accuracy: 0.8920\n",
      "Epoch 94/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.8182 - accuracy: 0.6618\n",
      "Epoch 93/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2387 - accuracy: 0.8923\n",
      "Epoch 97/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.8252 - accuracy: 0.6611\n",
      "Epoch 67/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2369 - accuracy: 0.8924\n",
      "Epoch 96/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2379 - accuracy: 0.8912\n",
      "Epoch 95/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2393 - accuracy: 0.8923\n",
      "Epoch 95/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6060 - accuracy: 0.6706\n",
      " 982/1076 [==========================>...] - ETA: 0s - loss: 0.2325 - accuracy: 0.8948Epoch 94/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2336 - accuracy: 0.8949\n",
      "Epoch 98/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.7414 - accuracy: 0.6645\n",
      "Epoch 68/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2379 - accuracy: 0.8929\n",
      "Epoch 97/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2530 - accuracy: 0.8892\n",
      " 338/1076 [========>.....................] - ETA: 1s - loss: 0.2236 - accuracy: 0.9026Epoch 96/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2665 - accuracy: 0.8861\n",
      "Epoch 96/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5421 - accuracy: 0.6700\n",
      "Epoch 95/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2330 - accuracy: 0.8946\n",
      "Epoch 99/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6635 - accuracy: 0.6643\n",
      " 997/1076 [==========================>...] - ETA: 0s - loss: 0.2378 - accuracy: 0.8917Epoch 69/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2394 - accuracy: 0.8912\n",
      "Epoch 98/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2324 - accuracy: 0.8940\n",
      "Epoch 97/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2382 - accuracy: 0.8922\n",
      " 451/1076 [===========>..................] - ETA: 1s - loss: 0.5360 - accuracy: 0.6708Epoch 97/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6272 - accuracy: 0.6698\n",
      "Epoch 96/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2294 - accuracy: 0.8965\n",
      "Epoch 100/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5738 - accuracy: 0.6648\n",
      "Epoch 70/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2417 - accuracy: 0.8925\n",
      "Epoch 99/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2297 - accuracy: 0.8934\n",
      "Epoch 98/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2306 - accuracy: 0.8946\n",
      "Epoch 98/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.8125 - accuracy: 0.6715\n",
      " 442/1076 [===========>..................] - ETA: 1s - loss: 0.2274 - accuracy: 0.8954Epoch 97/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2315 - accuracy: 0.8970\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.5476 - accuracy: 0.6587\n",
      "Epoch 71/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2326 - accuracy: 0.8945\n",
      "Epoch 100/100\n",
      "269/269 [==============================] - 1s 2ms/step - loss: 0.5384 - accuracy: 0.8347\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2344 - accuracy: 0.8918\n",
      "Epoch 99/100\n",
      " 368/1076 [=========>....................] - ETA: 1s - loss: 0.7823 - accuracy: 0.6623Epoch 1/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2361 - accuracy: 0.8918\n",
      "Epoch 99/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6774 - accuracy: 0.6666\n",
      "Epoch 98/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.7957 - accuracy: 0.6604\n",
      "Epoch 72/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2317 - accuracy: 0.8946\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2343 - accuracy: 0.8920\n",
      "Epoch 100/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2372 - accuracy: 0.8918\n",
      "214/269 [======================>.......] - ETA: 0s - loss: 0.4631 - accuracy: 0.8411Epoch 100/100\n",
      "269/269 [==============================] - 1s 2ms/step - loss: 0.4562 - accuracy: 0.8442\n",
      " 187/1076 [====>.........................] - ETA: 1s - loss: 0.2184 - accuracy: 0.9002Epoch 1/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 1.0246 - accuracy: 0.7632\n",
      " 872/1076 [=======================>......] - ETA: 0s - loss: 0.8495 - accuracy: 0.6694Epoch 2/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.8110 - accuracy: 0.6694\n",
      "Epoch 99/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.0784 - accuracy: 0.6643\n",
      "Epoch 73/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2345 - accuracy: 0.8923\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.2457 - accuracy: 0.8908\n",
      "269/269 [==============================] - 1s 2ms/step - loss: 0.5359 - accuracy: 0.8325\n",
      " 73/269 [=======>......................] - ETA: 0s - loss: 0.4466 - accuracy: 0.8399Epoch 1/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.4684 - accuracy: 0.7743\n",
      "147/269 [===============>..............] - ETA: 0s - loss: 0.4770 - accuracy: 0.8312Epoch 3/100\n",
      "269/269 [==============================] - 1s 1ms/step - loss: 0.4586 - accuracy: 0.8334\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5964 - accuracy: 0.6698\n",
      "Epoch 100/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5561 - accuracy: 0.6665\n",
      "Epoch 74/100\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 1.3790 - accuracy: 0.7409\n",
      "Epoch 2/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5049 - accuracy: 0.7585\n",
      "Epoch 4/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.0996 - accuracy: 0.6670\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.9927 - accuracy: 0.6610\n",
      "1032/1076 [===========================>..] - ETA: 0s - loss: 1.6803 - accuracy: 0.7308Epoch 75/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.6493 - accuracy: 0.7311\n",
      "Epoch 2/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.4397 - accuracy: 0.7908\n",
      "Epoch 3/100\n",
      "269/269 [==============================] - 0s 1ms/step - loss: 1.6035 - accuracy: 0.6716\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.5941 - accuracy: 0.7214\n",
      "Epoch 5/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 17.2569 - accuracy: 0.6574\n",
      "Epoch 76/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 3.8900 - accuracy: 0.6846\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1076/1076 [==============================] - 2s 1ms/step - loss: 0.4549 - accuracy: 0.7750\n",
      "Epoch 4/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 4.0378 - accuracy: 0.6927\n",
      "Epoch 6/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 4.7691 - accuracy: 0.6597\n",
      "Epoch 77/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 0.5903 - accuracy: 0.6733\n",
      "Epoch 4/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 0.4500 - accuracy: 0.7798\n",
      "Epoch 5/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 0.8802 - accuracy: 0.6622\n",
      "Epoch 7/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 0.8766 - accuracy: 0.6568\n",
      "Epoch 78/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 0.6062 - accuracy: 0.6749\n",
      "Epoch 5/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 0.4779 - accuracy: 0.7736\n",
      "Epoch 6/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5076 - accuracy: 0.6691\n",
      "Epoch 8/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5345 - accuracy: 0.6554\n",
      "Epoch 79/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5968 - accuracy: 0.6798\n",
      "Epoch 6/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6900 - accuracy: 0.7465\n",
      "Epoch 7/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5091 - accuracy: 0.6729\n",
      "Epoch 9/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.4889 - accuracy: 0.6525\n",
      "Epoch 80/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5965 - accuracy: 0.7004\n",
      "Epoch 7/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 2.0618 - accuracy: 0.7259\n",
      "Epoch 8/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 0.4927 - accuracy: 0.6819\n",
      "Epoch 10/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 0.4908 - accuracy: 0.6589\n",
      "Epoch 81/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 0.5703 - accuracy: 0.7476\n",
      "Epoch 8/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 1.5811 - accuracy: 0.6805\n",
      "Epoch 9/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 0.5456 - accuracy: 0.6914\n",
      "Epoch 11/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.4980 - accuracy: 0.6573\n",
      "Epoch 82/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.4997 - accuracy: 0.7579\n",
      "Epoch 9/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6660 - accuracy: 0.6852\n",
      "Epoch 10/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 0.5760 - accuracy: 0.6702\n",
      "Epoch 12/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 0.5926 - accuracy: 0.6567\n",
      "Epoch 83/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 0.5038 - accuracy: 0.7662\n",
      "Epoch 10/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 0.6139 - accuracy: 0.6951\n",
      "Epoch 11/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5976 - accuracy: 0.6744\n",
      "Epoch 13/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6001 - accuracy: 0.6543\n",
      "Epoch 84/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.4821 - accuracy: 0.7684\n",
      "Epoch 11/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6124 - accuracy: 0.6918\n",
      "Epoch 12/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6662 - accuracy: 0.6713\n",
      "Epoch 14/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.9921 - accuracy: 0.6556\n",
      "Epoch 85/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.4722 - accuracy: 0.7744\n",
      "Epoch 12/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5171 - accuracy: 0.6884\n",
      "Epoch 13/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5826 - accuracy: 0.6725\n",
      "Epoch 15/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.3158 - accuracy: 0.6510\n",
      "Epoch 86/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.4878 - accuracy: 0.7669\n",
      "Epoch 13/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5933 - accuracy: 0.6858\n",
      "Epoch 14/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6929 - accuracy: 0.6692\n",
      "Epoch 16/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.0864 - accuracy: 0.6544\n",
      "Epoch 87/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.4813 - accuracy: 0.7672\n",
      "Epoch 14/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6050 - accuracy: 0.6822\n",
      "Epoch 15/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6038 - accuracy: 0.6701\n",
      " 679/1076 [=================>............] - ETA: 0s - loss: 0.7486 - accuracy: 0.6538Epoch 17/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.7376 - accuracy: 0.6544\n",
      "Epoch 88/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 2.4778 - accuracy: 0.7368\n",
      "Epoch 15/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.7910 - accuracy: 0.6778\n",
      "Epoch 16/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.4103 - accuracy: 0.6680\n",
      "Epoch 18/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 0.7013 - accuracy: 0.6596\n",
      "Epoch 89/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 5.1781 - accuracy: 0.6836\n",
      "Epoch 16/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 1.1377 - accuracy: 0.6752\n",
      " 162/1076 [===>..........................] - ETA: 1s - loss: 0.4968 - accuracy: 0.6481Epoch 17/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.0938 - accuracy: 0.6664\n",
      "Epoch 19/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.7123 - accuracy: 0.6564\n",
      "Epoch 90/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.8069 - accuracy: 0.6760\n",
      "Epoch 17/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 1.4003 - accuracy: 0.6723\n",
      "Epoch 18/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 0.7112 - accuracy: 0.6707\n",
      "Epoch 20/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.0289 - accuracy: 0.6583\n",
      "Epoch 91/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7076 - accuracy: 0.6774\n",
      "Epoch 18/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.8734 - accuracy: 0.6737\n",
      "Epoch 19/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.8315 - accuracy: 0.6685\n",
      " 684/1076 [==================>...........] - ETA: 0s - loss: 0.6029 - accuracy: 0.6610Epoch 21/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6260 - accuracy: 0.6593\n",
      "Epoch 92/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5939 - accuracy: 0.6743\n",
      "Epoch 19/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7169 - accuracy: 0.6700\n",
      "Epoch 20/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.8482 - accuracy: 0.6626\n",
      "Epoch 22/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7541 - accuracy: 0.6595\n",
      "Epoch 93/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6155 - accuracy: 0.6758\n",
      "Epoch 20/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5471 - accuracy: 0.6715\n",
      "Epoch 21/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 8.4427 - accuracy: 0.6671\n",
      "Epoch 23/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.2910 - accuracy: 0.6575\n",
      "Epoch 94/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7566 - accuracy: 0.6748\n",
      "Epoch 21/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 6.8518 - accuracy: 0.6708\n",
      "Epoch 22/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.3540 - accuracy: 0.6669\n",
      "Epoch 24/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.6356 - accuracy: 0.6599\n",
      "Epoch 95/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6331 - accuracy: 0.6754\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.0096 - accuracy: 0.6690\n",
      "Epoch 23/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6163 - accuracy: 0.6701\n",
      "Epoch 25/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.9815 - accuracy: 0.6612\n",
      "Epoch 96/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6688 - accuracy: 0.6744\n",
      "Epoch 23/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7346 - accuracy: 0.6671\n",
      "Epoch 24/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5799 - accuracy: 0.6695\n",
      "Epoch 26/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5960 - accuracy: 0.6610\n",
      "Epoch 97/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5453 - accuracy: 0.6715\n",
      "Epoch 24/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6703 - accuracy: 0.6685\n",
      "Epoch 25/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5121 - accuracy: 0.6694\n",
      "Epoch 27/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5245 - accuracy: 0.6581\n",
      "Epoch 98/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5782 - accuracy: 0.6776\n",
      "Epoch 25/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6027 - accuracy: 0.6697\n",
      " 118/1076 [==>...........................] - ETA: 1s - loss: 0.5970 - accuracy: 0.6740Epoch 26/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5551 - accuracy: 0.6641\n",
      "Epoch 28/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 31.1793 - accuracy: 0.6610\n",
      "Epoch 99/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 0.8214 - accuracy: 0.6759\n",
      "Epoch 26/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 0.5946 - accuracy: 0.6662\n",
      "Epoch 27/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 0.8291 - accuracy: 0.6650\n",
      "Epoch 29/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 2.9769 - accuracy: 0.6586\n",
      "Epoch 100/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 1.1453 - accuracy: 0.6721\n",
      "Epoch 27/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 0.7290 - accuracy: 0.6703\n",
      "Epoch 28/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 1.0287 - accuracy: 0.6634\n",
      " 397/1076 [==========>...................] - ETA: 0s - loss: 0.6132 - accuracy: 0.6631Epoch 30/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6924 - accuracy: 0.6530\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.6029 - accuracy: 0.6766\n",
      "Epoch 28/100\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.7679 - accuracy: 0.6624\n",
      "Epoch 29/100\n",
      "269/269 [==============================] - 1s 1ms/step - loss: 12.3827 - accuracy: 0.6680\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.8228 - accuracy: 0.6649\n",
      "Epoch 31/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.1161 - accuracy: 0.6732\n",
      "Epoch 29/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.8287 - accuracy: 0.6616\n",
      "Epoch 30/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7391 - accuracy: 0.6685\n",
      "Epoch 32/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.0360 - accuracy: 0.6709\n",
      " 568/1076 [==============>...............] - ETA: 0s - loss: 0.6387 - accuracy: 0.6663Epoch 30/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6746 - accuracy: 0.6622\n",
      "Epoch 31/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6786 - accuracy: 0.6638\n",
      "Epoch 33/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6675 - accuracy: 0.6739\n",
      "Epoch 31/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5700 - accuracy: 0.6639\n",
      "Epoch 32/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5618 - accuracy: 0.6688\n",
      "Epoch 34/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6935 - accuracy: 0.6694\n",
      "Epoch 32/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5314 - accuracy: 0.6652\n",
      "Epoch 33/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.1404 - accuracy: 0.6660\n",
      "Epoch 35/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.8087 - accuracy: 0.6714\n",
      "Epoch 33/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6304 - accuracy: 0.6655\n",
      "Epoch 34/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.8583 - accuracy: 0.6664\n",
      "Epoch 36/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7958 - accuracy: 0.6675\n",
      "Epoch 34/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.8888 - accuracy: 0.6661\n",
      "Epoch 35/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7389 - accuracy: 0.6709\n",
      "Epoch 37/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6743 - accuracy: 0.6718\n",
      "Epoch 35/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.8343 - accuracy: 0.6655\n",
      "Epoch 36/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6032 - accuracy: 0.6687\n",
      "Epoch 38/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5675 - accuracy: 0.6729\n",
      "Epoch 36/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.2659 - accuracy: 0.6629\n",
      "Epoch 37/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6919 - accuracy: 0.6650\n",
      "Epoch 39/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 4.4705 - accuracy: 0.6746\n",
      "Epoch 37/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.2009 - accuracy: 0.6661\n",
      "Epoch 38/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6450 - accuracy: 0.6672\n",
      "Epoch 40/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 2.7473 - accuracy: 0.6766\n",
      "Epoch 38/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.8269 - accuracy: 0.6638\n",
      "Epoch 39/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7091 - accuracy: 0.6649\n",
      "Epoch 41/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7970 - accuracy: 0.6680\n",
      "Epoch 39/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.0380 - accuracy: 0.6665\n",
      "Epoch 40/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.9312 - accuracy: 0.6682\n",
      "Epoch 42/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6483 - accuracy: 0.6665\n",
      "Epoch 40/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.8193 - accuracy: 0.6658\n",
      "Epoch 41/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.3593 - accuracy: 0.6692\n",
      "Epoch 43/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5791 - accuracy: 0.6658\n",
      "Epoch 41/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 4.4275 - accuracy: 0.6620\n",
      "Epoch 42/100\n",
      "1076/1076 [==============================] - 2s 1ms/step - loss: 0.9006 - accuracy: 0.6620\n",
      "Epoch 44/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5718 - accuracy: 0.6710\n",
      "Epoch 42/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 2.2316 - accuracy: 0.6704\n",
      "Epoch 43/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6518 - accuracy: 0.6635\n",
      "Epoch 45/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6183 - accuracy: 0.6706\n",
      "Epoch 43/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6898 - accuracy: 0.6643\n",
      "Epoch 44/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.4994 - accuracy: 0.6657\n",
      "Epoch 46/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5992 - accuracy: 0.6683\n",
      "Epoch 44/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5329 - accuracy: 0.6619\n",
      "Epoch 45/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6375 - accuracy: 0.6689\n",
      "Epoch 47/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.2740 - accuracy: 0.6668\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6214 - accuracy: 0.6624\n",
      "Epoch 46/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6851 - accuracy: 0.6678\n",
      "Epoch 48/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7869 - accuracy: 0.6699\n",
      "Epoch 46/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5908 - accuracy: 0.6655\n",
      "Epoch 47/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.8249 - accuracy: 0.6670\n",
      "Epoch 49/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7152 - accuracy: 0.6667\n",
      "Epoch 47/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6633 - accuracy: 0.6603\n",
      "Epoch 48/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 11.2665 - accuracy: 0.6644\n",
      " 355/1076 [========>.....................] - ETA: 0s - loss: 0.5466 - accuracy: 0.6599Epoch 50/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7730 - accuracy: 0.6727\n",
      "Epoch 48/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5780 - accuracy: 0.6601\n",
      "Epoch 49/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.7000 - accuracy: 0.6620\n",
      "Epoch 51/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6343 - accuracy: 0.6679\n",
      "Epoch 49/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5810 - accuracy: 0.6604\n",
      "Epoch 50/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.0606 - accuracy: 0.6644\n",
      "Epoch 52/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7643 - accuracy: 0.6677\n",
      "Epoch 50/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.9877 - accuracy: 0.6576\n",
      "Epoch 51/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7986 - accuracy: 0.6614\n",
      "Epoch 53/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7060 - accuracy: 0.6724\n",
      "Epoch 51/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6350 - accuracy: 0.6595\n",
      "Epoch 52/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5834 - accuracy: 0.6613\n",
      "Epoch 54/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6352 - accuracy: 0.6649\n",
      "Epoch 52/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7514 - accuracy: 0.6600\n",
      "Epoch 53/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5208 - accuracy: 0.6653\n",
      "Epoch 55/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.3393 - accuracy: 0.6679\n",
      "Epoch 53/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.8794 - accuracy: 0.6608\n",
      "Epoch 54/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5451 - accuracy: 0.6651\n",
      "Epoch 56/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.1909 - accuracy: 0.6683\n",
      "Epoch 54/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6515 - accuracy: 0.6602\n",
      "Epoch 55/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7848 - accuracy: 0.6594\n",
      "Epoch 57/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6920 - accuracy: 0.6709\n",
      "Epoch 55/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.8727 - accuracy: 0.6644\n",
      "Epoch 56/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.8042 - accuracy: 0.6588\n",
      "Epoch 58/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7611 - accuracy: 0.6668\n",
      "Epoch 56/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.0145 - accuracy: 0.6612\n",
      "Epoch 57/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.9928 - accuracy: 0.6555\n",
      "Epoch 59/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.8057 - accuracy: 0.6693\n",
      "Epoch 57/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7132 - accuracy: 0.6590\n",
      "Epoch 58/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.0334 - accuracy: 0.6579\n",
      "Epoch 60/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7285 - accuracy: 0.6697\n",
      "Epoch 58/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7321 - accuracy: 0.6606\n",
      "Epoch 59/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.9186 - accuracy: 0.6555\n",
      "Epoch 61/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5935 - accuracy: 0.6695\n",
      "Epoch 59/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.1713 - accuracy: 0.6669\n",
      "Epoch 60/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.1253 - accuracy: 0.6542\n",
      "Epoch 62/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6396 - accuracy: 0.6705\n",
      "Epoch 60/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 8.8232 - accuracy: 0.6613\n",
      "Epoch 61/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7636 - accuracy: 0.6537\n",
      "Epoch 63/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5953 - accuracy: 0.6673\n",
      "Epoch 61/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.5296 - accuracy: 0.6642\n",
      " 789/1076 [====================>.........] - ETA: 0s - loss: 0.7084 - accuracy: 0.6581Epoch 62/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.8813 - accuracy: 0.6598\n",
      "Epoch 64/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.5840 - accuracy: 0.6640\n",
      "Epoch 62/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.0294 - accuracy: 0.6661\n",
      "Epoch 63/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.0521 - accuracy: 0.6558\n",
      "Epoch 65/100\n",
      "1076/1076 [==============================] - 1s 999us/step - loss: 0.6041 - accuracy: 0.6612\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.0663 - accuracy: 0.6673\n",
      "Epoch 64/100\n",
      "Epoch 63/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7462 - accuracy: 0.6589\n",
      "Epoch 66/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5142 - accuracy: 0.6584\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5839 - accuracy: 0.6732\n",
      "Epoch 65/100\n",
      "Epoch 64/100\n",
      "1076/1076 [==============================] - 1s 999us/step - loss: 7.9551 - accuracy: 0.6590\n",
      "Epoch 67/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5025 - accuracy: 0.6619\n",
      "Epoch 66/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.1708 - accuracy: 0.6667\n",
      "   1/1076 [..............................] - ETA: 1s - loss: 0.5739 - accuracy: 0.4688Epoch 65/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 3.4645 - accuracy: 0.6572\n",
      "Epoch 68/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5350 - accuracy: 0.6624\n",
      "Epoch 67/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 17.7550 - accuracy: 0.6692\n",
      "Epoch 66/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.8535 - accuracy: 0.6630\n",
      "Epoch 69/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6950 - accuracy: 0.6645\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 3.6709 - accuracy: 0.6661\n",
      "Epoch 68/100\n",
      "Epoch 67/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.9029 - accuracy: 0.6558\n",
      "Epoch 70/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.1556 - accuracy: 0.6638\n",
      "Epoch 68/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.8596 - accuracy: 0.6620\n",
      "Epoch 69/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5447 - accuracy: 0.6577\n",
      "Epoch 71/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7751 - accuracy: 0.6691\n",
      "Epoch 69/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.2379 - accuracy: 0.6606\n",
      "Epoch 70/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5062 - accuracy: 0.6543\n",
      "Epoch 72/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6680 - accuracy: 0.6659\n",
      "Epoch 70/100\n",
      "1076/1076 [==============================] - 1s 997us/step - loss: 0.9549 - accuracy: 0.6563\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5603 - accuracy: 0.6612\n",
      "Epoch 73/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.9401 - accuracy: 0.6594\n",
      "Epoch 72/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.0103 - accuracy: 0.6645\n",
      "Epoch 71/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7820 - accuracy: 0.6555\n",
      "Epoch 74/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.5778 - accuracy: 0.6563\n",
      "Epoch 73/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5905 - accuracy: 0.6640\n",
      "Epoch 72/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.9501 - accuracy: 0.6539\n",
      "Epoch 75/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.2685 - accuracy: 0.6618\n",
      "Epoch 74/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5495 - accuracy: 0.6655\n",
      "Epoch 73/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7709 - accuracy: 0.6570\n",
      "Epoch 76/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6445 - accuracy: 0.6581\n",
      "Epoch 75/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.8601 - accuracy: 0.6627\n",
      "Epoch 74/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.8855 - accuracy: 0.6587\n",
      "Epoch 77/100\n",
      "1076/1076 [==============================] - 1s 997us/step - loss: 0.5200 - accuracy: 0.6582\n",
      "Epoch 76/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.0993 - accuracy: 0.6611\n",
      "Epoch 75/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7671 - accuracy: 0.6552\n",
      "Epoch 78/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5201 - accuracy: 0.6629\n",
      "Epoch 77/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7939 - accuracy: 0.6680\n",
      "Epoch 76/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.9209 - accuracy: 0.6524\n",
      "Epoch 79/100\n",
      "1076/1076 [==============================] - 1s 991us/step - loss: 0.5403 - accuracy: 0.6651\n",
      "Epoch 78/100\n",
      "1076/1076 [==============================] - 1s 998us/step - loss: 0.6947 - accuracy: 0.6622\n",
      "Epoch 77/100\n",
      "1076/1076 [==============================] - 1s 996us/step - loss: 0.8541 - accuracy: 0.6555\n",
      "Epoch 80/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.6067 - accuracy: 0.6623\n",
      "Epoch 79/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5753 - accuracy: 0.6678\n",
      "Epoch 78/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6670 - accuracy: 0.6525\n",
      "Epoch 81/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.4465 - accuracy: 0.6663\n",
      "Epoch 80/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6362 - accuracy: 0.6630\n",
      "Epoch 79/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6369 - accuracy: 0.6529\n",
      "Epoch 82/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.9563 - accuracy: 0.6626\n",
      "Epoch 81/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6884 - accuracy: 0.6610\n",
      "Epoch 80/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.8661 - accuracy: 0.6576\n",
      "Epoch 83/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5753 - accuracy: 0.6618\n",
      "Epoch 82/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.1182 - accuracy: 0.6633\n",
      "Epoch 81/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5999 - accuracy: 0.6571\n",
      "Epoch 84/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.8610 - accuracy: 0.6601\n",
      "Epoch 83/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.0223 - accuracy: 0.6650\n",
      "Epoch 82/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.3778 - accuracy: 0.6575\n",
      "Epoch 85/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.0130 - accuracy: 0.6602\n",
      "Epoch 84/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.5496 - accuracy: 0.6621\n",
      "Epoch 83/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.3622 - accuracy: 0.6563\n",
      "Epoch 86/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.5183 - accuracy: 0.6664\n",
      "Epoch 85/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6317 - accuracy: 0.6630\n",
      "Epoch 84/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.3047 - accuracy: 0.6621\n",
      "Epoch 87/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6131 - accuracy: 0.6665\n",
      "Epoch 86/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7921 - accuracy: 0.6646\n",
      "Epoch 85/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.4904 - accuracy: 0.6560\n",
      "Epoch 88/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7925 - accuracy: 0.6614\n",
      "Epoch 87/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.2292 - accuracy: 0.6604\n",
      "Epoch 86/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 14.7303 - accuracy: 0.6572\n",
      "Epoch 89/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5579 - accuracy: 0.6657\n",
      "Epoch 88/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7040 - accuracy: 0.6596\n",
      "Epoch 87/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7320 - accuracy: 0.6574\n",
      "Epoch 90/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5177 - accuracy: 0.6632\n",
      "Epoch 89/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.8802 - accuracy: 0.6602\n",
      "Epoch 88/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5272 - accuracy: 0.6526\n",
      "Epoch 91/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5036 - accuracy: 0.6658\n",
      "Epoch 90/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6120 - accuracy: 0.6584\n",
      "Epoch 89/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6102 - accuracy: 0.6571\n",
      "Epoch 92/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 50.0052 - accuracy: 0.6630\n",
      "Epoch 91/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6772 - accuracy: 0.6652\n",
      "Epoch 90/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5870 - accuracy: 0.6557\n",
      "Epoch 93/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.2357 - accuracy: 0.6647\n",
      "Epoch 92/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.0911 - accuracy: 0.6655\n",
      "Epoch 91/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6219 - accuracy: 0.6598\n",
      "Epoch 94/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.0827 - accuracy: 0.6631\n",
      "Epoch 93/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.0209 - accuracy: 0.6661\n",
      "Epoch 92/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5883 - accuracy: 0.6628\n",
      "Epoch 95/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7949 - accuracy: 0.6656\n",
      "Epoch 94/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.8367 - accuracy: 0.6624\n",
      "Epoch 93/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.9183 - accuracy: 0.6570\n",
      "Epoch 96/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6617 - accuracy: 0.6628\n",
      "Epoch 95/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.8246 - accuracy: 0.6619\n",
      "Epoch 94/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.1176 - accuracy: 0.6554\n",
      "Epoch 97/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.8746 - accuracy: 0.6664\n",
      "Epoch 96/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 2.0372 - accuracy: 0.6660\n",
      "Epoch 95/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6692 - accuracy: 0.6530\n",
      "Epoch 98/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7598 - accuracy: 0.6625\n",
      "Epoch 97/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.0026 - accuracy: 0.6655\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.7157 - accuracy: 0.6607\n",
      "Epoch 99/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5617 - accuracy: 0.6642\n",
      "Epoch 98/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5551 - accuracy: 0.6691\n",
      "Epoch 97/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.2696 - accuracy: 0.6536\n",
      "Epoch 100/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.6656 - accuracy: 0.6669\n",
      "Epoch 99/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 0.5580 - accuracy: 0.6631\n",
      "Epoch 98/100\n",
      "1076/1076 [==============================] - 1s 1ms/step - loss: 1.0349 - accuracy: 0.6547\n",
      "269/269 [==============================] - 0s 820us/step - loss: 3.5473 - accuracy: 0.6620\n",
      "1076/1076 [==============================] - 1s 981us/step - loss: 0.5931 - accuracy: 0.6655\n",
      "Epoch 100/100\n",
      "1076/1076 [==============================] - 1s 981us/step - loss: 1.2061 - accuracy: 0.6692\n",
      "Epoch 99/100\n",
      "1076/1076 [==============================] - 1s 947us/step - loss: 0.5701 - accuracy: 0.6662\n",
      "1076/1076 [==============================] - 1s 947us/step - loss: 1.2243 - accuracy: 0.6649\n",
      "Epoch 100/100\n",
      "269/269 [==============================] - 0s 744us/step - loss: 7.7701 - accuracy: 0.6717\n",
      "1076/1076 [==============================] - 1s 870us/step - loss: 1.6268 - accuracy: 0.6651\n",
      "269/269 [==============================] - 0s 666us/step - loss: 3.8218 - accuracy: 0.6650\n",
      "Epoch 1/100\n",
      "1345/1345 [==============================] - 2s 864us/step - loss: 0.4086 - accuracy: 0.7944\n",
      "Epoch 2/100\n",
      "1345/1345 [==============================] - 1s 845us/step - loss: 0.3523 - accuracy: 0.8265\n",
      "Epoch 3/100\n",
      "1345/1345 [==============================] - 1s 829us/step - loss: 0.3384 - accuracy: 0.8354\n",
      "Epoch 4/100\n",
      "1345/1345 [==============================] - 1s 813us/step - loss: 0.3286 - accuracy: 0.8407\n",
      "Epoch 5/100\n",
      "1345/1345 [==============================] - 1s 819us/step - loss: 0.3228 - accuracy: 0.8430\n",
      "Epoch 6/100\n",
      "1345/1345 [==============================] - 1s 794us/step - loss: 0.3175 - accuracy: 0.8462\n",
      "Epoch 7/100\n",
      "1345/1345 [==============================] - 1s 784us/step - loss: 0.3130 - accuracy: 0.8496\n",
      "Epoch 8/100\n",
      "1345/1345 [==============================] - 1s 813us/step - loss: 0.3092 - accuracy: 0.8521\n",
      "Epoch 9/100\n",
      "1345/1345 [==============================] - 1s 780us/step - loss: 0.3062 - accuracy: 0.8533\n",
      "Epoch 10/100\n",
      "1345/1345 [==============================] - 1s 796us/step - loss: 0.3043 - accuracy: 0.8541\n",
      "Epoch 11/100\n",
      "1345/1345 [==============================] - 1s 849us/step - loss: 0.3003 - accuracy: 0.8570\n",
      "Epoch 12/100\n",
      "1345/1345 [==============================] - 1s 760us/step - loss: 0.2990 - accuracy: 0.8567\n",
      "Epoch 13/100\n",
      "1345/1345 [==============================] - 1s 767us/step - loss: 0.2963 - accuracy: 0.8591\n",
      "Epoch 14/100\n",
      "1345/1345 [==============================] - 1s 774us/step - loss: 0.2934 - accuracy: 0.8626\n",
      "Epoch 15/100\n",
      "1345/1345 [==============================] - 1s 771us/step - loss: 0.2923 - accuracy: 0.8624\n",
      "Epoch 16/100\n",
      "1345/1345 [==============================] - 1s 773us/step - loss: 0.2901 - accuracy: 0.8619\n",
      "Epoch 17/100\n",
      "1345/1345 [==============================] - 1s 1ms/step - loss: 0.2887 - accuracy: 0.8636\n",
      "Epoch 18/100\n",
      "1345/1345 [==============================] - 1s 967us/step - loss: 0.2860 - accuracy: 0.8653\n",
      "Epoch 19/100\n",
      "1345/1345 [==============================] - 1s 793us/step - loss: 0.2854 - accuracy: 0.8646\n",
      "Epoch 20/100\n",
      "1345/1345 [==============================] - 1s 760us/step - loss: 0.2827 - accuracy: 0.8675\n",
      "Epoch 21/100\n",
      "1345/1345 [==============================] - 1s 761us/step - loss: 0.2817 - accuracy: 0.8673\n",
      "Epoch 22/100\n",
      "1345/1345 [==============================] - 1s 785us/step - loss: 0.2811 - accuracy: 0.8679\n",
      "Epoch 23/100\n",
      "1345/1345 [==============================] - 1s 764us/step - loss: 0.2796 - accuracy: 0.8693\n",
      "Epoch 24/100\n",
      "1345/1345 [==============================] - 1s 760us/step - loss: 0.2779 - accuracy: 0.8695\n",
      "Epoch 25/100\n",
      "1345/1345 [==============================] - 1s 765us/step - loss: 0.2768 - accuracy: 0.8711\n",
      "Epoch 26/100\n",
      "1345/1345 [==============================] - 1s 778us/step - loss: 0.2766 - accuracy: 0.8707\n",
      "Epoch 27/100\n",
      "1345/1345 [==============================] - 1s 881us/step - loss: 0.2749 - accuracy: 0.8716\n",
      "Epoch 28/100\n",
      "1345/1345 [==============================] - 1s 863us/step - loss: 0.2736 - accuracy: 0.8727\n",
      "Epoch 29/100\n",
      "1345/1345 [==============================] - 1s 821us/step - loss: 0.2724 - accuracy: 0.8724\n",
      "Epoch 30/100\n",
      "1345/1345 [==============================] - 1s 810us/step - loss: 0.2707 - accuracy: 0.8737\n",
      "Epoch 31/100\n",
      "1345/1345 [==============================] - 1s 753us/step - loss: 0.2706 - accuracy: 0.8740\n",
      "Epoch 32/100\n",
      "1345/1345 [==============================] - 1s 752us/step - loss: 0.2694 - accuracy: 0.8749\n",
      "Epoch 33/100\n",
      "1345/1345 [==============================] - 1s 772us/step - loss: 0.2686 - accuracy: 0.8758\n",
      "Epoch 34/100\n",
      "1345/1345 [==============================] - 1s 839us/step - loss: 0.2680 - accuracy: 0.8755\n",
      "Epoch 35/100\n",
      "1345/1345 [==============================] - 1s 742us/step - loss: 0.2669 - accuracy: 0.8750\n",
      "Epoch 36/100\n",
      "1345/1345 [==============================] - 1s 759us/step - loss: 0.2669 - accuracy: 0.8766\n",
      "Epoch 37/100\n",
      "1345/1345 [==============================] - 1s 828us/step - loss: 0.2655 - accuracy: 0.8767\n",
      "Epoch 38/100\n",
      "1345/1345 [==============================] - 1s 886us/step - loss: 0.2653 - accuracy: 0.8764\n",
      "Epoch 39/100\n",
      "1345/1345 [==============================] - 1s 778us/step - loss: 0.2641 - accuracy: 0.8779\n",
      "Epoch 40/100\n",
      "1345/1345 [==============================] - 1s 731us/step - loss: 0.2637 - accuracy: 0.8792\n",
      "Epoch 41/100\n",
      "1345/1345 [==============================] - 1s 792us/step - loss: 0.2624 - accuracy: 0.8788\n",
      "Epoch 42/100\n",
      "1345/1345 [==============================] - 1s 727us/step - loss: 0.2614 - accuracy: 0.8794\n",
      "Epoch 43/100\n",
      "1345/1345 [==============================] - 1s 773us/step - loss: 0.2611 - accuracy: 0.8802\n",
      "Epoch 44/100\n",
      "1345/1345 [==============================] - 1s 765us/step - loss: 0.2604 - accuracy: 0.8796\n",
      "Epoch 45/100\n",
      "1345/1345 [==============================] - 1s 741us/step - loss: 0.2599 - accuracy: 0.8801\n",
      "Epoch 46/100\n",
      "1345/1345 [==============================] - 1s 769us/step - loss: 0.2589 - accuracy: 0.8797\n",
      "Epoch 47/100\n",
      "1345/1345 [==============================] - 1s 939us/step - loss: 0.2588 - accuracy: 0.8809\n",
      "Epoch 48/100\n",
      "1345/1345 [==============================] - 1s 751us/step - loss: 0.2584 - accuracy: 0.8810\n",
      "Epoch 49/100\n",
      "1345/1345 [==============================] - 1s 847us/step - loss: 0.2585 - accuracy: 0.8803\n",
      "Epoch 50/100\n",
      "1345/1345 [==============================] - 1s 847us/step - loss: 0.2569 - accuracy: 0.8819\n",
      "Epoch 51/100\n",
      "1345/1345 [==============================] - 1s 737us/step - loss: 0.2572 - accuracy: 0.8823\n",
      "Epoch 52/100\n",
      "1345/1345 [==============================] - 1s 747us/step - loss: 0.2558 - accuracy: 0.8834\n",
      "Epoch 53/100\n",
      "1345/1345 [==============================] - 1s 834us/step - loss: 0.2561 - accuracy: 0.8830\n",
      "Epoch 54/100\n",
      "1345/1345 [==============================] - 1s 842us/step - loss: 0.2551 - accuracy: 0.8824\n",
      "Epoch 55/100\n",
      "1345/1345 [==============================] - 1s 851us/step - loss: 0.2537 - accuracy: 0.8832\n",
      "Epoch 56/100\n",
      "1345/1345 [==============================] - 1s 870us/step - loss: 0.2549 - accuracy: 0.8835\n",
      "Epoch 57/100\n",
      "1345/1345 [==============================] - 1s 800us/step - loss: 0.2543 - accuracy: 0.8829\n",
      "Epoch 58/100\n",
      "1345/1345 [==============================] - 1s 803us/step - loss: 0.2531 - accuracy: 0.8844\n",
      "Epoch 59/100\n",
      "1345/1345 [==============================] - 1s 793us/step - loss: 0.2529 - accuracy: 0.8836\n",
      "Epoch 60/100\n",
      "1345/1345 [==============================] - 1s 789us/step - loss: 0.2520 - accuracy: 0.8843\n",
      "Epoch 61/100\n",
      "1345/1345 [==============================] - 1s 802us/step - loss: 0.2517 - accuracy: 0.8846\n",
      "Epoch 62/100\n",
      "1345/1345 [==============================] - 1s 764us/step - loss: 0.2510 - accuracy: 0.8856\n",
      "Epoch 63/100\n",
      "1345/1345 [==============================] - 1s 764us/step - loss: 0.2505 - accuracy: 0.8861\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1345/1345 [==============================] - 1s 789us/step - loss: 0.2499 - accuracy: 0.8856\n",
      "Epoch 65/100\n",
      "1345/1345 [==============================] - 1s 754us/step - loss: 0.2505 - accuracy: 0.8842\n",
      "Epoch 66/100\n",
      "1345/1345 [==============================] - 1s 759us/step - loss: 0.2492 - accuracy: 0.8861\n",
      "Epoch 67/100\n",
      "1345/1345 [==============================] - 1s 768us/step - loss: 0.2497 - accuracy: 0.8860\n",
      "Epoch 68/100\n",
      "1345/1345 [==============================] - 1s 734us/step - loss: 0.2489 - accuracy: 0.8865\n",
      "Epoch 69/100\n",
      "1345/1345 [==============================] - 1s 737us/step - loss: 0.2489 - accuracy: 0.8861\n",
      "Epoch 70/100\n",
      "1345/1345 [==============================] - 1s 746us/step - loss: 0.2481 - accuracy: 0.8863\n",
      "Epoch 71/100\n",
      "1345/1345 [==============================] - 1s 756us/step - loss: 0.2488 - accuracy: 0.8859\n",
      "Epoch 72/100\n",
      "1345/1345 [==============================] - 1s 737us/step - loss: 0.2468 - accuracy: 0.8874\n",
      "Epoch 73/100\n",
      "1345/1345 [==============================] - 1s 767us/step - loss: 0.2480 - accuracy: 0.8869\n",
      "Epoch 74/100\n",
      "1345/1345 [==============================] - 1s 768us/step - loss: 0.2470 - accuracy: 0.8869\n",
      "Epoch 75/100\n",
      "1345/1345 [==============================] - 1s 725us/step - loss: 0.2462 - accuracy: 0.8885\n",
      "Epoch 76/100\n",
      "1345/1345 [==============================] - 1s 801us/step - loss: 0.2458 - accuracy: 0.8879\n",
      "Epoch 77/100\n",
      "1345/1345 [==============================] - 1s 768us/step - loss: 0.2455 - accuracy: 0.8887\n",
      "Epoch 78/100\n",
      "1345/1345 [==============================] - 1s 720us/step - loss: 0.2454 - accuracy: 0.8888\n",
      "Epoch 79/100\n",
      "1345/1345 [==============================] - 1s 744us/step - loss: 0.2454 - accuracy: 0.8879\n",
      "Epoch 80/100\n",
      "1345/1345 [==============================] - 1s 720us/step - loss: 0.2449 - accuracy: 0.8871\n",
      "Epoch 81/100\n",
      "1345/1345 [==============================] - 1s 721us/step - loss: 0.2448 - accuracy: 0.8879\n",
      "Epoch 82/100\n",
      "1345/1345 [==============================] - 1s 720us/step - loss: 0.2441 - accuracy: 0.8895\n",
      "Epoch 83/100\n",
      "1345/1345 [==============================] - 1s 720us/step - loss: 0.2435 - accuracy: 0.8877\n",
      "Epoch 84/100\n",
      "1345/1345 [==============================] - 1s 727us/step - loss: 0.2438 - accuracy: 0.8878\n",
      "Epoch 85/100\n",
      "1345/1345 [==============================] - 1s 734us/step - loss: 0.2435 - accuracy: 0.8887\n",
      "Epoch 86/100\n",
      "1345/1345 [==============================] - 1s 736us/step - loss: 0.2439 - accuracy: 0.8888\n",
      "Epoch 87/100\n",
      "1345/1345 [==============================] - 1s 736us/step - loss: 0.2429 - accuracy: 0.8889\n",
      "Epoch 88/100\n",
      "1345/1345 [==============================] - 1s 718us/step - loss: 0.2426 - accuracy: 0.8900\n",
      "Epoch 89/100\n",
      "1345/1345 [==============================] - 1s 719us/step - loss: 0.2424 - accuracy: 0.8900\n",
      "Epoch 90/100\n",
      "1345/1345 [==============================] - 1s 719us/step - loss: 0.2415 - accuracy: 0.8898\n",
      "Epoch 91/100\n",
      "1345/1345 [==============================] - 1s 719us/step - loss: 0.2417 - accuracy: 0.8897\n",
      "Epoch 92/100\n",
      "1345/1345 [==============================] - 1s 763us/step - loss: 0.2414 - accuracy: 0.8900\n",
      "Epoch 93/100\n",
      "1345/1345 [==============================] - 1s 841us/step - loss: 0.2410 - accuracy: 0.8901\n",
      "Epoch 94/100\n",
      "1345/1345 [==============================] - 1s 910us/step - loss: 0.2419 - accuracy: 0.8891\n",
      "Epoch 95/100\n",
      "1345/1345 [==============================] - 1s 912us/step - loss: 0.2404 - accuracy: 0.8914\n",
      "Epoch 96/100\n",
      "1345/1345 [==============================] - 1s 895us/step - loss: 0.2399 - accuracy: 0.8911\n",
      "Epoch 97/100\n",
      "1345/1345 [==============================] - 1s 870us/step - loss: 0.2399 - accuracy: 0.8905\n",
      "Epoch 98/100\n",
      "1345/1345 [==============================] - 1s 748us/step - loss: 0.2395 - accuracy: 0.8921\n",
      "Epoch 99/100\n",
      "1345/1345 [==============================] - 1s 720us/step - loss: 0.2402 - accuracy: 0.8920\n",
      "Epoch 100/100\n",
      "1345/1345 [==============================] - 1s 716us/step - loss: 0.2396 - accuracy: 0.8900\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m best_model \u001b[38;5;241m=\u001b[39m grid_result\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Evaluate the best model on the validation set\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m val_accuracy \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mscore(\u001b[43mval_data\u001b[49m, val_labels)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_data' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Define a function to build the neural network model with variable epochs\n",
    "def create_model(epochs=10, num_units=64, learning_rate=0.001):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(num_units, activation='relu', input_shape=(d_in,)),\n",
    "        keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the KerasClassifier for use in scikit-learn\n",
    "model = KerasClassifier(build_fn=create_model, batch_size=32, verbose=1)\n",
    "\n",
    "# Define the hyperparameter grid to search over\n",
    "param_grid = {\n",
    "    'epochs': [100],\n",
    "    'num_units': [64],\n",
    "    'learning_rate': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=6)\n",
    "grid_result = grid_search.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Get the best hyperparameters and the best model\n",
    "best_params = grid_result.best_params_\n",
    "best_model = grid_result.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the validation set\n",
    "val_accuracy = best_model.score(val_data, val_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "577/577 [==============================] - 0s 564us/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_nnw = best_model.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_nnw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.83      0.84      9219\n",
      "           1       0.83      0.86      0.85      9216\n",
      "\n",
      "    accuracy                           0.85     18435\n",
      "   macro avg       0.85      0.85      0.85     18435\n",
      "weighted avg       0.85      0.85      0.85     18435\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metricas(y_pred_nnw, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://1a1d36e8-daa5-4977-8bcd-1341c8e913c2/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['neuralnetwork.joblib']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(best_model, \"neuralnetwork.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: test.pkl/assets\n"
     ]
    }
   ],
   "source": [
    "best_model.model.save(\"test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
